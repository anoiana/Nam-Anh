{
  "articles": [
    {
      "path": "index.html",
      "title": "Bayesian Statistics",
      "description": "\"Begin with an estimate of the probability that any claim, belief, hypothesis is true, then look at any new data and update the probability given the new data.\"\n\n-Steven Novella-\n",
      "author": [],
      "contents": "\n\n“No mud, no lotus”\n\n\n“Smile to the cloud in your tea”\n\n\nThich Nhat Hanh\n\n\n\n\n",
      "last_modified": "2022-03-19T18:07:15-04:00"
    },
    {
      "path": "z-example-maic.html",
      "title": "An Example of MAIC",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWe delineate an example in which we’d like to compare two treatments A and C. However, only individual-patient data of treatment A and B are available, and treatment C is only available through a previous study that compare treatment B and C. All information of treatment C reported in the study comprises mean and standard deviation of outcomes and baseline characteristics, so this type of data is usually called aggregate data. For example,\n\n\nhide\n\nset.seed(1234)\nipd = tibble(\n     outcome = rnorm(100, mean = 10),\n     trt = sample(c(\"treatmentA\",\"placebo\"),100, replace = T),\n     age = rpois(100,30),\n     gender = sample(c(\"F\",\"M\"),100, prob = c(0.3,0.7), replace = T),\n     weight = rgamma(100,70,1.2),\n     region = sample(c(\"urban\",\"rural\",\"suburban\"), 100, replace = T, prob = c(0.5,0.4,0.1))\n)\n\nipd|> MyTable(\"Individual patient data.\", TRUE, format = \"html\")\n\n\n\n\nTable 1: Individual patient data.\n\n\noutcome\n\n\ntrt\n\n\nage\n\n\ngender\n\n\nweight\n\n\nregion\n\n\n8.792934\n\n\nplacebo\n\n\n27\n\n\nM\n\n\n59.67280\n\n\nurban\n\n\n10.277429\n\n\ntreatmentA\n\n\n29\n\n\nM\n\n\n59.82493\n\n\nrural\n\n\n11.084441\n\n\nplacebo\n\n\n29\n\n\nM\n\n\n58.97129\n\n\nurban\n\n\n7.654302\n\n\ntreatmentA\n\n\n43\n\n\nM\n\n\n64.86729\n\n\nurban\n\n\n10.429125\n\n\ntreatmentA\n\n\n34\n\n\nF\n\n\n65.13997\n\n\nurban\n\n\n10.506056\n\n\ntreatmentA\n\n\n32\n\n\nM\n\n\n55.07439\n\n\nrural\n\n\n9.425260\n\n\nplacebo\n\n\n28\n\n\nF\n\n\n58.08483\n\n\nrural\n\n\n9.453368\n\n\nplacebo\n\n\n25\n\n\nM\n\n\n54.46514\n\n\nrural\n\n\n9.435548\n\n\nplacebo\n\n\n27\n\n\nM\n\n\n66.54571\n\n\nurban\n\n\n9.109962\n\n\ntreatmentA\n\n\n35\n\n\nF\n\n\n64.03493\n\n\nurban\n\n\n9.522807\n\n\nplacebo\n\n\n24\n\n\nM\n\n\n61.34866\n\n\nrural\n\n\n9.001614\n\n\ntreatmentA\n\n\n28\n\n\nM\n\n\n57.04962\n\n\nrural\n\n\n9.223746\n\n\nplacebo\n\n\n32\n\n\nM\n\n\n66.00696\n\n\nsuburban\n\n\n10.064459\n\n\ntreatmentA\n\n\n33\n\n\nF\n\n\n53.49538\n\n\nurban\n\n\n10.959494\n\n\ntreatmentA\n\n\n39\n\n\nM\n\n\n69.87203\n\n\nurban\n\n\n9.889715\n\n\ntreatmentA\n\n\n29\n\n\nF\n\n\n47.01138\n\n\nurban\n\n\n9.488990\n\n\nplacebo\n\n\n28\n\n\nM\n\n\n61.83252\n\n\nurban\n\n\n9.088805\n\n\ntreatmentA\n\n\n38\n\n\nF\n\n\n57.30711\n\n\nsuburban\n\n\n9.162828\n\n\ntreatmentA\n\n\n39\n\n\nF\n\n\n68.87114\n\n\nrural\n\n\n12.415835\n\n\ntreatmentA\n\n\n30\n\n\nF\n\n\n53.30723\n\n\nrural\n\n\n10.134088\n\n\nplacebo\n\n\n28\n\n\nF\n\n\n56.43860\n\n\nurban\n\n\n9.509314\n\n\nplacebo\n\n\n20\n\n\nM\n\n\n69.92221\n\n\nurban\n\n\n9.559452\n\n\nplacebo\n\n\n20\n\n\nM\n\n\n49.71859\n\n\nurban\n\n\n10.459589\n\n\ntreatmentA\n\n\n23\n\n\nF\n\n\n64.74365\n\n\nrural\n\n\n9.306280\n\n\ntreatmentA\n\n\n38\n\n\nM\n\n\n66.13664\n\n\nrural\n\n\n8.551795\n\n\ntreatmentA\n\n\n33\n\n\nF\n\n\n58.61712\n\n\nsuburban\n\n\n10.574756\n\n\ntreatmentA\n\n\n34\n\n\nF\n\n\n66.28978\n\n\nrural\n\n\n8.976344\n\n\nplacebo\n\n\n30\n\n\nM\n\n\n52.72887\n\n\nrural\n\n\n9.984862\n\n\ntreatmentA\n\n\n27\n\n\nM\n\n\n48.91692\n\n\nurban\n\n\n9.064051\n\n\nplacebo\n\n\n32\n\n\nM\n\n\n54.38834\n\n\nsuburban\n\n\n11.102298\n\n\nplacebo\n\n\n34\n\n\nF\n\n\n59.66526\n\n\nurban\n\n\n9.524407\n\n\ntreatmentA\n\n\n40\n\n\nM\n\n\n57.32527\n\n\nsuburban\n\n\n9.290560\n\n\ntreatmentA\n\n\n36\n\n\nF\n\n\n56.35687\n\n\nurban\n\n\n9.498742\n\n\ntreatmentA\n\n\n27\n\n\nM\n\n\n59.11837\n\n\nurban\n\n\n8.370907\n\n\ntreatmentA\n\n\n23\n\n\nF\n\n\n72.44018\n\n\nurban\n\n\n8.832381\n\n\nplacebo\n\n\n27\n\n\nM\n\n\n49.94096\n\n\nrural\n\n\n7.819960\n\n\ntreatmentA\n\n\n23\n\n\nF\n\n\n57.13733\n\n\nurban\n\n\n8.659007\n\n\nplacebo\n\n\n32\n\n\nF\n\n\n49.33394\n\n\nurban\n\n\n9.705706\n\n\nplacebo\n\n\n41\n\n\nM\n\n\n71.96957\n\n\nurban\n\n\n9.534102\n\n\nplacebo\n\n\n32\n\n\nM\n\n\n57.88031\n\n\nurban\n\n\n11.449496\n\n\ntreatmentA\n\n\n33\n\n\nM\n\n\n55.15301\n\n\nurban\n\n\n8.931357\n\n\nplacebo\n\n\n24\n\n\nM\n\n\n68.31172\n\n\nurban\n\n\n9.144635\n\n\ntreatmentA\n\n\n30\n\n\nF\n\n\n49.32812\n\n\nurban\n\n\n9.719377\n\n\ntreatmentA\n\n\n28\n\n\nM\n\n\n63.74633\n\n\nurban\n\n\n9.005660\n\n\nplacebo\n\n\n28\n\n\nM\n\n\n64.10039\n\n\nsuburban\n\n\n9.031486\n\n\ntreatmentA\n\n\n34\n\n\nM\n\n\n68.48832\n\n\nsuburban\n\n\n8.892682\n\n\nplacebo\n\n\n25\n\n\nM\n\n\n49.31132\n\n\nrural\n\n\n8.748014\n\n\ntreatmentA\n\n\n31\n\n\nM\n\n\n59.54409\n\n\nrural\n\n\n9.476172\n\n\nplacebo\n\n\n34\n\n\nM\n\n\n57.64155\n\n\nurban\n\n\n9.503150\n\n\nplacebo\n\n\n40\n\n\nM\n\n\n60.90532\n\n\nurban\n\n\n8.193969\n\n\ntreatmentA\n\n\n36\n\n\nF\n\n\n62.56447\n\n\nurban\n\n\n9.417924\n\n\ntreatmentA\n\n\n30\n\n\nF\n\n\n64.90112\n\n\nrural\n\n\n8.891110\n\n\ntreatmentA\n\n\n23\n\n\nM\n\n\n66.59768\n\n\nurban\n\n\n8.985038\n\n\ntreatmentA\n\n\n37\n\n\nM\n\n\n67.91815\n\n\nrural\n\n\n9.837691\n\n\ntreatmentA\n\n\n23\n\n\nM\n\n\n59.96693\n\n\nrural\n\n\n10.563056\n\n\ntreatmentA\n\n\n21\n\n\nM\n\n\n60.57957\n\n\nurban\n\n\n11.647818\n\n\nplacebo\n\n\n26\n\n\nM\n\n\n59.28004\n\n\nurban\n\n\n9.226647\n\n\nplacebo\n\n\n29\n\n\nM\n\n\n59.88316\n\n\nurban\n\n\n11.605910\n\n\ntreatmentA\n\n\n26\n\n\nF\n\n\n59.20598\n\n\nsuburban\n\n\n8.842192\n\n\ntreatmentA\n\n\n38\n\n\nF\n\n\n54.34046\n\n\nurban\n\n\n10.656588\n\n\nplacebo\n\n\n31\n\n\nM\n\n\n64.94346\n\n\nrural\n\n\n12.548991\n\n\nplacebo\n\n\n36\n\n\nF\n\n\n60.13112\n\n\nurban\n\n\n9.965240\n\n\ntreatmentA\n\n\n30\n\n\nF\n\n\n47.91994\n\n\nrural\n\n\n9.330366\n\n\nplacebo\n\n\n30\n\n\nM\n\n\n55.49349\n\n\nurban\n\n\n9.992395\n\n\ntreatmentA\n\n\n31\n\n\nM\n\n\n73.05632\n\n\nrural\n\n\n11.777084\n\n\nplacebo\n\n\n19\n\n\nM\n\n\n45.57260\n\n\nurban\n\n\n8.861392\n\n\nplacebo\n\n\n45\n\n\nM\n\n\n63.53059\n\n\nrural\n\n\n11.367827\n\n\nplacebo\n\n\n33\n\n\nM\n\n\n61.13577\n\n\nsuburban\n\n\n11.329565\n\n\nplacebo\n\n\n26\n\n\nM\n\n\n61.72055\n\n\nrural\n\n\n10.336473\n\n\ntreatmentA\n\n\n20\n\n\nM\n\n\n58.01844\n\n\nrural\n\n\n10.006893\n\n\nplacebo\n\n\n28\n\n\nM\n\n\n51.72460\n\n\nrural\n\n\n9.544531\n\n\nplacebo\n\n\n24\n\n\nM\n\n\n66.05686\n\n\nrural\n\n\n9.633476\n\n\ntreatmentA\n\n\n28\n\n\nM\n\n\n54.05132\n\n\nrural\n\n\n10.648287\n\n\nplacebo\n\n\n30\n\n\nM\n\n\n51.83709\n\n\nurban\n\n\n12.070271\n\n\nplacebo\n\n\n26\n\n\nM\n\n\n56.52850\n\n\nurban\n\n\n9.846602\n\n\ntreatmentA\n\n\n28\n\n\nM\n\n\n68.52850\n\n\nurban\n\n\n8.609299\n\n\ntreatmentA\n\n\n30\n\n\nM\n\n\n50.38738\n\n\nurban\n\n\n9.276418\n\n\nplacebo\n\n\n33\n\n\nF\n\n\n64.04940\n\n\nrural\n\n\n10.258262\n\n\nplacebo\n\n\n30\n\n\nM\n\n\n56.89783\n\n\nsuburban\n\n\n9.682941\n\n\ntreatmentA\n\n\n32\n\n\nM\n\n\n55.95564\n\n\nurban\n\n\n9.822210\n\n\ntreatmentA\n\n\n23\n\n\nM\n\n\n63.64407\n\n\nrural\n\n\n9.830006\n\n\nplacebo\n\n\n32\n\n\nM\n\n\n56.14802\n\n\nrural\n\n\n8.627698\n\n\nplacebo\n\n\n24\n\n\nF\n\n\n54.26577\n\n\nurban\n\n\n9.826213\n\n\ntreatmentA\n\n\n37\n\n\nM\n\n\n69.52284\n\n\nrural\n\n\n10.850232\n\n\ntreatmentA\n\n\n32\n\n\nM\n\n\n61.12066\n\n\nurban\n\n\n10.697609\n\n\ntreatmentA\n\n\n31\n\n\nF\n\n\n52.25626\n\n\nurban\n\n\n10.549997\n\n\ntreatmentA\n\n\n31\n\n\nM\n\n\n49.64264\n\n\nrural\n\n\n9.597268\n\n\ntreatmentA\n\n\n32\n\n\nM\n\n\n51.94375\n\n\nurban\n\n\n9.808406\n\n\ntreatmentA\n\n\n35\n\n\nM\n\n\n59.07608\n\n\nurban\n\n\n8.805472\n\n\nplacebo\n\n\n32\n\n\nF\n\n\n62.57839\n\n\nrural\n\n\n9.946841\n\n\ntreatmentA\n\n\n33\n\n\nM\n\n\n58.71110\n\n\nrural\n\n\n10.255196\n\n\nplacebo\n\n\n23\n\n\nF\n\n\n69.45260\n\n\nurban\n\n\n11.705964\n\n\nplacebo\n\n\n38\n\n\nM\n\n\n58.72222\n\n\nurban\n\n\n11.001513\n\n\ntreatmentA\n\n\n31\n\n\nM\n\n\n49.53246\n\n\nrural\n\n\n9.504417\n\n\ntreatmentA\n\n\n34\n\n\nF\n\n\n70.48453\n\n\nurban\n\n\n10.355550\n\n\nplacebo\n\n\n22\n\n\nM\n\n\n56.20976\n\n\nrural\n\n\n8.865392\n\n\ntreatmentA\n\n\n35\n\n\nF\n\n\n52.42653\n\n\nurban\n\n\n10.878204\n\n\ntreatmentA\n\n\n28\n\n\nM\n\n\n73.38835\n\n\nsuburban\n\n\n10.972917\n\n\nplacebo\n\n\n30\n\n\nF\n\n\n58.46773\n\n\nrural\n\n\n12.121117\n\n\ntreatmentA\n\n\n20\n\n\nM\n\n\n67.37558\n\n\nurban\n\n\n\nWe could obtain first and second moments of continuous variables and proportions of discrete variables through Table 1. Thus, mean and variance of age and weight are shown in the following table\n\n\nhide\n\nd1<-\nipd[,c(\"age\",\"weight\")]|>\n     vapply(\\(x) c(\"mean\" = mean(x),\"variance\"=var(x)), rep(NA_real_,2))\nd1\n\n\n              age   weight\nmean     30.18000 59.56129\nvariance 29.92687 43.85138\n\nand proportions of gender and region are\n\n\nhide\n\nd2<-\nipd[,c(\"gender\",\"region\")]|>\n     lapply(\\(x) table(x)/sum(table(x)))\nd2\n\n\n$gender\nx\n   F    M \n0.32 0.68 \n\n$region\nx\n   rural suburban    urban \n    0.37     0.11     0.52 \n\nAnother type of data is aggregate data which can be found in many articles. The aggregate data disclose only summarized statistics, such as mean, standard deviation (continuous variables), proportions (discrete variables). For example, We obtain summarized statistics from an article in which treatmentB is compared to placebo as follows\n\n\nhide\n\nd3 = matrix(c(35,25,62,47.5), ncol = 2)%>%\n     `colnames<-`(c(\"age\",\"weight\"))%>%\n     `row.names<-`(c(\"mean\",\"variance\"))\nd3\n\n\n         age weight\nmean      35   62.0\nvariance  25   47.5\n\nhide\n\nd4 = list('gender' = c(\"F\" = 0.4,\"M\" = 0.6), 'region' = c(\"rural\" = 0.4,\"suburban\" = 0.2, \"urban\" = 0.4))\nd4\n\n\n$gender\n  F   M \n0.4 0.6 \n\n$region\n   rural suburban    urban \n     0.4      0.2      0.4 \n\nAlso, we obtain different mean and its variance (usually calculated from CI) are 15 and 2, respectively.\n\n\n\n",
      "last_modified": "2022-03-19T18:07:20-04:00"
    },
    {
      "path": "z-generate-sample-with-rjags.html",
      "title": "Generate Samples with `Rjags`",
      "description": "This note summarizes methods to generate samples and calculate measures of interest through such generated samples.",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1 Sampling with rjags\n1.1 Generalized t distribution\n1.2 Transformation\n1.3 A more complicated problem\n\n2 Prediction with unknown parameters\n3 Running MCMC using rjags\n\n1 Sampling with rjags\n1.1 Generalized t distribution\nSuppose we want to generate samples from generalized \\(t\\) defined as follows\n\\[\\begin{equation}\nf(x) = \\frac{c\\sigma^{-1}}{\\bigg[n+\\Big(\\frac{x-\\theta}{\\sigma}\\Big)^2\\bigg]^{0.5(n+1)}},\n\\end{equation}\\]\nwe say \\(X \\sim t(\\theta,\\sigma,n)\\), where \\(\\theta, \\sigma\\) and \\(n\\) are location, scale parameter and degree of freedom.\nTo generate samples of \\(X\\), we could use rt function in R to simulate from standard \\(t\\) distribution and then transform such samples. We, however, now wants to simulate generalized \\(t\\) distribution directly, which does not require an extra step of transformation. To this end, we employ rjags which is popular for obtaining samples from posterior distribution in Bayesian framework.\nIt is worth noting that the generalized \\(t\\) distribution function dt in rjags requires mean and precision rather than location and scale as its parameters. For example, we simulate samples from a generalized \\(t\\) distribution whose mean and variance are 10 and 0.5, we shall need to plug in values of 10 and 1/0.5 as its first two parameters (third parameter is degree of freedom which equals 4).\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n Y ~ dt(10,2,4)\n}\")\nmodel <- jags.model(model_string, n.chains = 1, quiet = T)\n#update(model, 1000, progress.bar=\"none\")\nparams <- c(\"Y\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=1000, progress.bar=\"none\")\n\n##################\nx = samples[[1]][,1] %>% sort()\n\n\n\nLet us compare with samples obtained by R function\n\n\nhide\n\ncurve(wiqid::dt3(x,10, sqrt(0.5),4), xlim = c(0,20), lty = 3,, ylab = \"density\")\npoints(x,wiqid::dt3(x,10,sqrt(0.5),4), col = \"red\", type = \"l\", lty = 2)\n\n\n\n\n1.2 Transformation\nWe sometimes require simulating samples from a transformed random variable, i.e we generate \\(Y = (2Z+1)^3\\), where \\(Z \\sim \\mathcal{N}(0,1)\\) and calculate \\(\\mathbb{E}(Y)\\) and \\(\\mathbb{P}(Y \\ge10)\\). Such measures can be obtained analytically, more specifically \\(\\mathbb{E}(Y) = 13\\) and \\(\\mathbb{P}(Y \\ge 10) = 0.28\\).1\nWe now use rjags to simulate samples\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n Z ~ dnorm(0,1)\n Y <- pow(2*Z+1,3)\n P<- step(Y-10)\n}\")\nmodel <- jags.model(model_string, n.chains = 1, quiet = T)\nupdate(model, 1000, progress.bar=\"none\")\nparams <- c(\"Y\",\"P\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=2000, progress.bar=\"none\")\n\n##################\n\nexp_val = mean(samples[[1]][,2])\nprob = mean(samples[[1]][,1])\n\n\n\nand we obtained \\(\\mathbb{E}(Y) =\\) 13.4886726 and \\(\\mathbb{P}(Y\\ge 10) =\\) 0.2955.\n1.3 A more complicated problem\nSuppose we are required calculating how many items can be fixed with $1000 given the cost to repair an item follows gamma distribution with mean \\(\\mu = 100\\) and standard deviation \\(\\sigma = 50\\), i.e\n\\[\\begin{equation}\nX \\sim \\mathcal{G}am(4, 0.04)\n\\end{equation}\\]\nwe implement as follows\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n  for(i in 1:n){Y[i] ~ dgamma(4,0.04)}\n  sim[1]<- Y[1]\n  for(i in 2:n){sim[i] <- sim[i-1]+Y[i]}\n  for(i in 1:n){order[i]<- step(1000-sim[i])}\n  item<- sum(order[])\n  check <- step(sim[n]-1000)\n  \n}\")\ndata = list(n=20)\nmodel <- jags.model(model_string, n.chains = 1, data = data, quiet = T)\nupdate(model, 1000, progress.bar=\"none\")\nparams <- c(\"item\",\"check\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=10000, progress.bar=\"none\")\n\n##################\n\nitem = samples[[1]][,2] %>% mean()\ncheck = samples[[1]][,1] %>% mean()\n\n\n\nMean of item is 9.6503 indicates there are about 10 items which can be repaired, Mean of check equals 1 indicates \\(n=20\\) is sufficient.\n2 Prediction with unknown parameters\nfor the sake of ease, we shall consider a mixed distribution as follows\n\\[\\begin{equation}\nX = K\\mathcal{N}(0,1) + (1-K)\\mathcal{N}(5,1),\n\\end{equation}\\]\nwhere \\(K \\sim \\mathcal{B}er(0.3)\\). Alternatively, above distribution can be rewritten as follows\n\\[\\begin{equation}\nf(x) = \\int f(y|k)f(k)dk,\n\\tag{1}\n\\end{equation}\\]\nwhich is predictive prior distribution, where \\(k\\) was defined above, and \\(Y|K=1 \\sim \\mathcal{N}(0,1)\\) while \\(Y|K=0 \\sim \\mathcal{N}(5,1)\\). To simulate samples of above distribution, we employ Bernoulli and Normal distributions as follows\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\nalpha ~ dbin(0.3,1)\nx1 ~ dnorm(0,1)\nx2 ~ dnorm(5,1)\n# x = alpha*x1 + (1-alpha)*x2\nx = ifelse(alpha==1,x1,x2)\n}\")\n\ndata = list(n=20)\nmodel <- jags.model(model_string, n.chains = 1, quiet = T)\nupdate(model, 1000, progress.bar=\"none\")\nparams <- c(\"x\",\"alpha\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=10000, progress.bar=\"none\")\n\nsamples[[1]][,2] %>% hist()\n\n\n\n(1) can be expended on distribution of \\(K\\) that is known as prior distribution. For example, we want to estimate, of 20 new students, how many ones will join painting club given the number of people joining the painting club follows Binomial distribution whose the second parameter follows Beta distribution. i.e\n\\[\\begin{equation}\nX \\sim \\mathcal{B}in(20,\\pi), \\quad \\pi \\sim \\mathcal{B}eta(3,2).\n\\end{equation}\\]\nThus, in rjags code one has\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n  p ~ dbeta(3,10)\n  x ~ dbin(p,20)}\"\n  \n  )\n\nmodel <- jags.model(model_string, n.chains = 1, quiet = T)\nupdate(model, 1000, progress.bar=\"none\")\nparams <- c(\"x\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=10000, progress.bar=\"none\")\n\nsummary(samples)\n\n\n\nIterations = 1001:11000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       4.61900        2.90477        0.02905        0.02905 \n\n2. Quantiles for each variable:\n\n 2.5%   25%   50%   75% 97.5% \n    0     2     4     6    11 \n\nhence, 5 of 20 new students will join the club. If we’d like to calculate probability that there is at least 2 students who will join the club, the result is\n\n\nhide\n\nmean(samples[[1]][,1]>=5)\n\n\n[1] 0.4621\n\n3 Running MCMC using rjags\nWe shall consider the following setup of a linear model:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i,\\sigma^2), \\text{ and} \\\\\n\\mu_i &= \\alpha+\\beta X_i, \\text{ where } i= 1,\\dots,N \\\\\n\\alpha,\\beta &\\sim \\mathcal{N}(0,h^2) \\\\\n\\ln{\\sigma} &\\sim \\mathcal{U}nif(-k,k) \\\\\n\\text{we choose } h &= 100, \\text{ and } k = 5000\n\\end{aligned}\n\\]\n(this example refers to.)2\nWe shall generate fake data for the model with \\(\\alpha = 150\\) and \\(\\beta=-4\\).\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n  # likelihood\n  for(i in 1:N){\n  y[i] ~ dnorm(mu[i],tau)\n  mu[i] <- alpha + beta*x[i]\n  }\n  \n  # redundant parameter \n  lsigma ~ dunif(-k,k)\n  tau <- pow(exp(lsigma),-2)\n  #sigma = pow(tau,-1)\n  \n  # hyperparameters\n  alpha ~ dnorm(142,prec)\n  beta ~ dnorm(0,prec)\n  prec <- pow(h,-2)\n  }\")\n\n\n# data setup\nset.seed(12345)\nx=rnorm(100,50,9)\nerror=rnorm(100,0,16)\ny=150-(4*x)+error\n\n\ndat = list(\"N\" = length(y), h = 100, k = 5000, y =y, x = x)\nparams = c(\"alpha\",\"beta\")\ninits = function(){list(alpha=rnorm(1),beta=rnorm(1),lsigma=rnorm(1))}\n\n\nmodel <- jags.model(model_string, data =  dat, inits = inits, n.chains = 2, quiet = T)\nupdate(model, 10000, progress.bar=\"none\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=4000, progress.bar=\"none\")\n\nsamps = purrr::lift_dl(rbind)(samples)\n\napply(samps,2,HDInterval::hdi, credMass = 0.95)%>%\n     purrr::compose(~rownames_to_column(.,\"params\"),as.data.frame,t)()%>%\n     mutate(mean = apply(samps,2,mean), .before = \"lower\")|>\n     mutate(params = recode(params, alpha = \"$\\\\alpha$\", beta = '$\\\\beta$'))|>\n     MyTable(\"95% highest density intervals.\")\n\n\n\nTable 1: 95% highest density intervals.\n\n\nparams\n\n\nmean\n\n\nlower\n\n\nupper\n\n\n\\(\\alpha\\)\n\n\n141.807096\n\n\n123.783771\n\n\n158.612193\n\n\n\\(\\beta\\)\n\n\n-3.829302\n\n\n-4.139125\n\n\n-3.485452\n\n\nTable 1 shows the 95% highest density intervals of two parameters \\(\\alpha\\) and \\(\\beta\\).\nAnother deserved consideration is convergence diagnostic. To archive this, the number of chains n.chains should be greater than 1, we chose n.chains = 2 in above example.\nIf we fitted the linear model by the frequentist approach, we would have\n\n\nhide\n\nout = lm(y~x)|>\n     broom::tidy()\ntibble(\n     params = c(\"$\\\\alpha$\", \"$\\\\beta$\"),\n     mean = c(out$estimate),\n     lower = c(out$estimate-qnorm(0.975)*out$std.error),\n     upper = c(out$estimate+qnorm(0.975)*out$std.error))|>\n     MyTable(\"95% confidence interval.\")\n\n\n\nTable 2: 95% confidence interval.\n\n\nparams\n\n\nmean\n\n\nlower\n\n\nupper\n\n\n\\(\\alpha\\)\n\n\n141.949740\n\n\n125.070610\n\n\n158.828871\n\n\n\\(\\beta\\)\n\n\n-3.831938\n\n\n-4.149498\n\n\n-3.514378\n\n\n\n\n\n1. Fouley J-L. The BUGS book: A practical introduction to bayesian analysis.\n\n\n2. Baio G. Bayesian methods in health economics. CRC Press Boca Raton, 2013.\n\n\n\n\n",
      "last_modified": "2022-03-19T18:07:31-04:00"
    },
    {
      "path": "z-limit-of-sup-inf.html",
      "title": "Limit of Supremum and Infimum",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1 \\(\\pmb{\\lim\\sup}\\) and \\(\\pmb{\\lim\\inf}\\) of a sequence of real numbers\n1.1 \\(\\lim\\inf\\)\n1.2 \\(\\lim\\sup\\)\n\n2 \\(\\lim\\sup\\) and \\(\\lim\\inf\\) of a sequence of sets\n\n1 \\(\\pmb{\\lim\\sup}\\) and \\(\\pmb{\\lim\\inf}\\) of a sequence of real numbers\nSuppose we have a bounded sequence of real numbers \\((c_n)^{\\infty}_{n=1}\\). Also define \\[\n\\begin{aligned}\na_n &:= \\inf\\{c_k: k \\ge n\\}, \\\\\nb_n &:= \\sup\\{c_k:k \\ge n\\}.\n\\end{aligned}\n\\] We deem two cases that likely occur with \\((c_n)_{n=1}^{\\infty}\\)\n1.1 \\(\\lim\\inf\\)\nIf \\((a_n)\\) is bounded and increasing, then it has a limit called \\(a\\). We can define the so-called \\(\\lim\\inf\\) of such a sequence as follows \\[\n\\lim_{n}\\inf(c_n) = \\lim_{n \\to \\infty}\\inf\\{c_k: k \\ge n\\} \n\\]\nThe following figure illustrates for \\(\\lim\\inf\\):\n\n\nhide\n\nf = Vectorize(function(n) (-1)^n*(n+5)/n)\nn = 1:30\ns = f(n)\nd = cbind(n,s)\nset.seed(200)\nr = runif(100,-1,0)\nplot(d[d[,2]<0,], pch = 16, ylim = c(-6,0), cex = 1.2, ylab = \"sequence (c)\",type = \"o\")\npoints(r, pch = 16, col = \"red\")\nabline(h = -1, col = \"red\")\n\n\n\n\nFigure 1: example for \\(\\lim\\inf\\)\n\n\n\nas \\(n\\) goes to \\(\\infty\\), \\((c_n)\\) approaches to \\(-1\\) (never reach \\(-1\\)). The term inf or infimum is also called the greatest lower bound, this is, the greatest point of lower bound set. We can see that all black filled circle are less than \\(-1\\), so all of them can be deemed lower bound of the collection of red filled circles, and a point corresponding \\(n\\) as \\(n \\to \\infty\\) is the greatest point. By definition, this is called infimum or the greatest lower bound.\n1.2 \\(\\lim\\sup\\)\nSimilarly,\nIf \\((b_n)\\) is bounded and decreasing, it has a limit called \\(b\\). We can define the so-called \\(\\lim\\sup\\) of such a sequence as follows \\[\n\\lim_n\\sup(c_n) = \\lim_{n \\to \\infty}\\sup\\{c_k: k \\ge n\\}.\n\\]\nThe following figure illustrates for sup or supremum\n\n\nhide\n\nset.seed(200)\nr = runif(100,0,1)\nplot(d[d[,2]>0,], pch = 16, cex = 1.2, ylab = \"sequence (c)\", ylim = c(0,3.5),type = \"o\")\npoints(r, pch = 16, col = \"red\")\nabline(h = 1, col = \"red\")\n\n\n\n\nFigure 2: example for \\(\\lim\\sup\\)\n\n\n\nIn this case, all black filled circles are greater than \\(1\\), the sequence goes to \\(1\\) as \\(n\\) approaches to \\(\\infty\\) (never reach \\(1\\)). Thus, they are upper bounds of the collection of red filled circles. Also, a point corresponding \\(n\\) as \\(n \\to \\infty\\) is the least point of the upper bound set. This is called supremum or the least upper bound.\nIf we combine both figures, we obtain\n\n\nhide\n\nset.seed(200)\nr = runif(500,-1,1)\nplot(d[d[,2]<0,], pch = 16, ylim = c(-6,6), cex = 0.7, ylab = \"sequence (c)\", type = \"o\")\npoints(d[d[,2]>0,], pch = 16, cex = 0.7, ylab = \"sequence (c)\", ylim = c(0,3.5), type = \"o\")\npoints(r, pch = 16, col = \"red\")\nabline(h = c(-1,1), col = \"red\")\n\n\n\n\nFigure 3: example for \\(\\lim\\sup\\)\n\n\n\nTwo horizontal red lines represent for \\(a_n\\) and \\(b_n\\), and when \\(n\\) goes to \\(\\infty\\) they approach to \\(-1\\) and \\(1\\), respectively. If we extend the interval \\([-1,1]\\) to a general interval \\([a,b]\\), then \\([a_{n+1},b_{n+1}]\\) will be contained in \\([a_n,b_n]\\) for all \\(n\\). We can say \\(\\lim_{n \\to \\infty}[a_n,b_n] = [-1,1]\\). This is, as \\(n \\to \\infty\\) it approaches to the shortest length interval of \\([a_n,b_n]_{n=1}^{\\infty}\\).\nWe can also express the above idea by intersection, ie., the intersection of all intervals \\([a_n,b_n]_{n = 1,2,\\dots}\\) is \\([a,b] \\equiv [-1,1]\\). In other words, intersection can be utilized to express the “smallest” magnitude of length, area, volume, etc. This idea can be extended to limits of sets.\n2 \\(\\lim\\sup\\) and \\(\\lim\\inf\\) of a sequence of sets\nLet \\(A_n \\subset \\Omega\\), we define\n\\[\n\\inf_{k \\ge n}A_k := \\bigcap_{k=n}^{\\infty}A_k; \\quad \\sup_{k \\ge n}A_k := \\bigcup_{k=n}^{\\infty}A_k\n\\tag{1}\n\\]\nthus,\n\\[\\begin{align}\n&\\lim_{n \\to \\infty}\\inf A_n = \\bigcup_{n=1}^{\\infty}\\bigcap_{k=n}^{\\infty}A_k, \\\\\n&\\lim_{n \\to \\infty}\\sup A_n = \\bigcap_{n=1}^{\\infty}\\bigcup_{k=n}^{\\infty}A_k\n\\tag{2}\n\\end{align}\\]\nTo get the intuition about \\(\\lim\\sup\\) and \\(\\lim\\inf\\) in term of union and intersection, let us consider Eq.(1) in details. We will consider the case of intersection, second case can be explained in the same way.\nSuppose \\(n= 10\\),\n\\[\n\\begin{aligned}\n&B_1 = \\bigcap_{i=1}^{10}A_i \\\\\n&B_2 = \\bigcap_{i=2}^{10}A_i \\\\\n&\\quad \\dots \\\\\n&B_9 = A_9 \\cap A_{10} \\\\\n&B_{10} = A_{10}\n\\end{aligned}\n\\]\nhence we can conclude\n\\[\nB_1 \\le B_2 \\le \\dots \\le B_{10}\n\\]\nand hence \\(B_n\\) is an increasing sequence, so it has a limit. What we need is to find a set that corresponds to greatest lower bound. It is NOT akin to the sequence of real numbers, The sequence of sets i.e., the magnitude of sets is determined by the intersection and union of all available sets. Thus, to find the greatest set, we can’t only compare among \\(\\{B_i\\}_{i=1}^{10}\\), but we also compare all intersections and unions among \\(\\{B_i\\}_{i=1}^{10}\\). To this end, the greatest set is\n\\[\n\\bigcup_{i=1}^{10}B_i = \\bigcup_{n=1}^{10}\\bigcap_{i=n}^{10}A_i,\n\\]\nwhich has a general form in Eq.(2). The case of supremum can be explained with an analogous manner. To this end, we have\n\\[\n\\begin{aligned}\n&B_1 = \\bigcup_{i=1}^{10}A_i \\\\\n&B_2 = \\bigcup_{i=2}^{10}A_i \\\\\n&\\quad \\dots \\\\\n&B_9 = A_9 \\cup A_{10} \\\\\n&B_{10} = A_{10}\n\\end{aligned}\n\\] and then \\[\nB_1 \\ge B_2 \\ge \\dots \\ge B_{10}\n\\]\nhence \\(B_n\\) is an decreasing sequence and it has a limit. With an analogous manner, we can obtain the smallest set that is \\[\n\\bigcap_{i=1}^{10}B_i = \\bigcap_{n=1}^{10}\\bigcup_{i=n}^{10}A_i.\n\\] which is second equation in Eq.(2).\n\n\n\n",
      "last_modified": "2022-03-19T18:07:35-04:00"
    },
    {
      "path": "z-proof-of-maic.html",
      "title": "The Proof of MAIC",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nLet \\(\\boldsymbol{p} = (p_1,p_2,\\dots,p_n)^{\\top}\\) be propensity score and \\(\\boldsymbol{x} = (x_1,x_2,\\dots,x_n)^{\\top}\\) baseline characteristics of subjects in IPD. Also, \\(\\mu\\) denotes average vector of baseline characteristics found in SLD. We aim to estimate \\(\\boldsymbol{p}\\) such that \\(\\boldsymbol{p}^{\\top}\\boldsymbol{x}-\\mu=0\\), i.e. we want to minimize Kullback Leibler divergence of \\(\\boldsymbol{p}\\) and empirical function with some constraints. Specifically, we want to minimize\n\\[\n\\mathcal{L} = -\\boldsymbol{p}^{\\top}\\ln(\\frac{1}{n\\boldsymbol{p}}),\n\\]\nwith two constraints:\n\\[\n\\boldsymbol{1}^{\\top}\\boldsymbol{p} = 1,\\, and\\quad \\boldsymbol{p}^{\\top}\\boldsymbol{x} =\\mu.\n\\]\nAlternatively, we optimize\n\\[\n\\mathcal{L} = -\\boldsymbol{p}^{\\top}\\ln(\\frac{1}{n\\boldsymbol{p}}) - \\alpha_0\\Big(\\sum_{i=1}^np_i -1\\Big) - \\alpha_1(\\boldsymbol{p}^{\\top}\\boldsymbol{x}-\\mu)\n\\tag{1}\n\\]\nEquation (1) could be solved by taking derivative w.r.t \\(\\boldsymbol{p}\\), \\(\\alpha_0\\) and \\(\\alpha_1\\) followed by equating derivations to zero and solve equations.\nTaking derivative w.r.t \\(p_i\\):\n\\[\n\\begin{aligned}\n&\\quad \\frac{\\partial\\mathcal{L}}{\\partial p_i} = 0 \\\\\n&\\Rightarrow \\ln(np_i) +1 -\\alpha_0 -\\alpha_1(x_i) = 0 \\\\\n&\\Rightarrow \\ln(np_i) = \\alpha_1(x_i)+\\alpha_0 -1 \\\\\n&\\Rightarrow p_i = \\frac{\\exp[\\alpha_0+\\alpha_1(x_i)-1]}{n} \\\\\n&\\Rightarrow p_i = \\frac{e^{-1}}{n}\\exp[\\alpha_0+ \\alpha_1(x_i)]\n\\end{aligned}\n\\tag{2}\n\\]\nAlso,\n\\[\n\\begin{aligned}\n&\\quad 1 = \\sum_{i=1}^np_i = \\frac{e^{-1}}{n}\\sum_{i=1}^n\\exp[\\alpha_0+\\alpha_1(x_i)] \\\\\n& \\Rightarrow \\frac{n}{e^{-1}} = \\sum_{i=1}^n\\exp[\\alpha_0+ \\alpha_1(x_i)]\n\\end{aligned}\n\\tag{3}\n\\]\nsubstitute (3) to (2), we obtain\n\\[\np_i = \\frac{\\exp[\\alpha_0+\\alpha_1(x_i)]}{\\sum_{i=1}^n\\exp[\\alpha_0 +\\alpha_1(x_i)]}\n\\] which is equivalent to\n\\[\np_i = \\frac{\\exp[\\alpha_1(x_i-\\mu)]}{\\sum_{i=1}^n\\exp[\\alpha_1(x_i-\\mu)]}\n\\tag{4}\n\\]\nLet \\(z_i = x_i-\\mu\\), and hence \\(p_i = \\frac{\\exp[\\alpha_1 z_i]}{\\sum_{i=1}^n\\exp[\\alpha_1 z_i]}\\), implying the weight\n\\[\nw_i = p_i\\sum_{j=1}^n\\exp[\\alpha_1z_j] = \\exp (\\alpha_1z_i)> 0.\n\\]\nThus,\n\\[\n\\sum_{i=1}^nw_iz_i = \\sum_{i=1}^n\\Big(p_iz_i\\sum_{j=1}^n\\exp[\\alpha_1z_j]\\Big) = \\sum_{i=1}^n\\frac{\\partial}{\\partial\\alpha_1}\\exp (\\alpha_1z_i)> 0,\n\\] integrating both sides w.r.t \\(\\alpha_1\\)\n\\[\n\\sum_{j=1}^n\\exp(\\alpha_1z_j)\\sum_{i=1}^np_iz_i + C = \\sum_{i=1}^n\\exp (\\alpha_1z_i)> 0,\n\\tag{5}\n\\] where \\(C\\) is a positive constant.\nRecall that \\(z_i = x_i-\\mu\\) and we are interested in \\(\\sum_{i=1}^np_ix_i = \\mu\\) implying that\n\\[\n\\sum_{i=1}^n p_iz_i= 0.\n\\tag{6}\n\\]\nAlso, the second term in (5) can be rewritten as\n\\[\nk\\sum_{i=1}^np_iz_i+C\n\\tag{7}\n\\]\nwhere \\(k = \\sum_{j=1}^n\\exp[\\alpha_1z_j]\\). Thus, the condition (6) is satisfied as the equation (7) equals \\(C\\), which is equivalent to minimizing the right hand side of (5), \\(\\sum_{i=1}^n\\exp (\\alpha_1z_i)\\ge 0\\), which can be rewritten in the matrix form when there are more than one effect modifiers\n\\[\n\\boldsymbol{1}^{\\top}\\exp(\\boldsymbol{Z\\alpha}_1) = \\sum_{i=1}^n\\exp(\\boldsymbol{z}_i^{\\top}\\boldsymbol{\\alpha}_1).\n\\]\nFor the sake of illustration, we shall consider the following example. Suppose we have effect modifiers age and gender of 100 patients as follows\n\n\nset.seed(1234)\nage = rnorm(100,50,5)\ngender = sample(c(0,1),100, replace = T)\n\ndata.frame(patient= 1:100,age = age, gender = gender)|>\n     MyTable(scroll_box = T, cap = \"Age and Gender of 100 patients. For *Gender*: 1: Female; 0: Male.\")\n\n\n\n\nTable 1: Age and Gender of 100 patients. For Gender: 1: Female; 0: Male.\n\n\npatient\n\n\nage\n\n\ngender\n\n\n1\n\n\n43.96467\n\n\n1\n\n\n2\n\n\n51.38715\n\n\n0\n\n\n3\n\n\n55.42221\n\n\n1\n\n\n4\n\n\n38.27151\n\n\n0\n\n\n5\n\n\n52.14562\n\n\n0\n\n\n6\n\n\n52.53028\n\n\n0\n\n\n7\n\n\n47.12630\n\n\n1\n\n\n8\n\n\n47.26684\n\n\n1\n\n\n9\n\n\n47.17774\n\n\n1\n\n\n10\n\n\n45.54981\n\n\n0\n\n\n11\n\n\n47.61404\n\n\n1\n\n\n12\n\n\n45.00807\n\n\n0\n\n\n13\n\n\n46.11873\n\n\n1\n\n\n14\n\n\n50.32229\n\n\n0\n\n\n15\n\n\n54.79747\n\n\n0\n\n\n16\n\n\n49.44857\n\n\n0\n\n\n17\n\n\n47.44495\n\n\n1\n\n\n18\n\n\n45.44402\n\n\n0\n\n\n19\n\n\n45.81414\n\n\n0\n\n\n20\n\n\n62.07918\n\n\n0\n\n\n21\n\n\n50.67044\n\n\n1\n\n\n22\n\n\n47.54657\n\n\n1\n\n\n23\n\n\n47.79726\n\n\n1\n\n\n24\n\n\n52.29795\n\n\n0\n\n\n25\n\n\n46.53140\n\n\n0\n\n\n26\n\n\n42.75898\n\n\n0\n\n\n27\n\n\n52.87378\n\n\n0\n\n\n28\n\n\n44.88172\n\n\n1\n\n\n29\n\n\n49.92431\n\n\n0\n\n\n30\n\n\n45.32026\n\n\n1\n\n\n31\n\n\n55.51149\n\n\n1\n\n\n32\n\n\n47.62203\n\n\n0\n\n\n33\n\n\n46.45280\n\n\n0\n\n\n34\n\n\n47.49371\n\n\n0\n\n\n35\n\n\n41.85453\n\n\n0\n\n\n36\n\n\n44.16190\n\n\n1\n\n\n37\n\n\n39.09980\n\n\n0\n\n\n38\n\n\n43.29503\n\n\n1\n\n\n39\n\n\n48.52853\n\n\n1\n\n\n40\n\n\n47.67051\n\n\n1\n\n\n41\n\n\n57.24748\n\n\n0\n\n\n42\n\n\n44.65679\n\n\n1\n\n\n43\n\n\n45.72318\n\n\n0\n\n\n44\n\n\n48.59689\n\n\n0\n\n\n45\n\n\n45.02830\n\n\n1\n\n\n46\n\n\n45.15743\n\n\n0\n\n\n47\n\n\n44.46341\n\n\n1\n\n\n48\n\n\n43.74007\n\n\n0\n\n\n49\n\n\n47.38086\n\n\n1\n\n\n50\n\n\n47.51575\n\n\n1\n\n\n51\n\n\n40.96984\n\n\n0\n\n\n52\n\n\n47.08962\n\n\n0\n\n\n53\n\n\n44.45555\n\n\n0\n\n\n54\n\n\n44.92519\n\n\n0\n\n\n55\n\n\n49.18845\n\n\n0\n\n\n56\n\n\n52.81528\n\n\n0\n\n\n57\n\n\n58.23909\n\n\n1\n\n\n58\n\n\n46.13323\n\n\n1\n\n\n59\n\n\n58.02955\n\n\n0\n\n\n60\n\n\n44.21096\n\n\n0\n\n\n61\n\n\n53.28294\n\n\n1\n\n\n62\n\n\n62.74496\n\n\n1\n\n\n63\n\n\n49.82620\n\n\n0\n\n\n64\n\n\n46.65183\n\n\n1\n\n\n65\n\n\n49.96198\n\n\n0\n\n\n66\n\n\n58.88542\n\n\n1\n\n\n67\n\n\n44.30696\n\n\n1\n\n\n68\n\n\n56.83914\n\n\n1\n\n\n69\n\n\n56.64782\n\n\n1\n\n\n70\n\n\n51.68236\n\n\n0\n\n\n71\n\n\n50.03446\n\n\n1\n\n\n72\n\n\n47.72266\n\n\n1\n\n\n73\n\n\n48.16738\n\n\n0\n\n\n74\n\n\n53.24143\n\n\n1\n\n\n75\n\n\n60.35135\n\n\n1\n\n\n76\n\n\n49.23301\n\n\n0\n\n\n77\n\n\n43.04650\n\n\n0\n\n\n78\n\n\n46.38209\n\n\n1\n\n\n79\n\n\n51.29131\n\n\n1\n\n\n80\n\n\n48.41470\n\n\n0\n\n\n81\n\n\n49.11105\n\n\n0\n\n\n82\n\n\n49.15003\n\n\n1\n\n\n83\n\n\n43.13849\n\n\n1\n\n\n84\n\n\n49.13106\n\n\n0\n\n\n85\n\n\n54.25116\n\n\n0\n\n\n86\n\n\n53.48804\n\n\n0\n\n\n87\n\n\n52.74999\n\n\n0\n\n\n88\n\n\n47.98634\n\n\n0\n\n\n89\n\n\n49.04203\n\n\n0\n\n\n90\n\n\n44.02736\n\n\n1\n\n\n91\n\n\n49.73421\n\n\n0\n\n\n92\n\n\n51.27598\n\n\n1\n\n\n93\n\n\n58.52982\n\n\n1\n\n\n94\n\n\n55.00757\n\n\n0\n\n\n95\n\n\n47.52208\n\n\n0\n\n\n96\n\n\n51.77775\n\n\n1\n\n\n97\n\n\n44.32696\n\n\n0\n\n\n98\n\n\n54.39102\n\n\n0\n\n\n99\n\n\n54.86458\n\n\n1\n\n\n100\n\n\n60.60559\n\n\n0\n\n\n\nMean of age and proportion of female are \\(\\hat{\\theta}_{age}\\) = 49.2161913 and \\(\\hat{\\theta}_{sex}\\) = 0.45, respectively. We shall match both \\(\\hat{\\theta}_{age}\\) and \\(\\hat{\\theta}_{sex}\\) to 52 and 0.5. To this end, we follow above instruction to determine \\(\\alpha_1\\) and \\(p_i\\). Thus,\n\n\nZ = matrix(c(age-age2,gender-gender2), ncol = 2)\nobjfn = function(alpha,Z) sum(exp(Z%*%alpha))\nalpha = optim(par = c(0,0), fn = objfn, Z = Z)$par\np = exp(Z%*%alpha)|>\n     {\\(x) x/sum(x)}()\n\ndata.frame(patient= 1:100,age = age, gender = gender, p = p)|>\n     MyTable(scroll_box = T, cap = \"Age and Gender and MAIC proportion of 100 patients.\")\n\n\n\n\nTable 2: Age and Gender and MAIC proportion of 100 patients.\n\n\npatient\n\n\nage\n\n\ngender\n\n\np\n\n\n1\n\n\n43.96467\n\n\n1\n\n\n0.0055922\n\n\n2\n\n\n51.38715\n\n\n0\n\n\n0.0102778\n\n\n3\n\n\n55.42221\n\n\n1\n\n\n0.0170282\n\n\n4\n\n\n38.27151\n\n\n0\n\n\n0.0028730\n\n\n5\n\n\n52.14562\n\n\n0\n\n\n0.0110641\n\n\n6\n\n\n52.53028\n\n\n0\n\n\n0.0114855\n\n\n7\n\n\n47.12630\n\n\n1\n\n\n0.0076037\n\n\n8\n\n\n47.26684\n\n\n1\n\n\n0.0077082\n\n\n9\n\n\n47.17774\n\n\n1\n\n\n0.0076418\n\n\n10\n\n\n45.54981\n\n\n0\n\n\n0.0058281\n\n\n11\n\n\n47.61404\n\n\n1\n\n\n0.0079728\n\n\n12\n\n\n45.00807\n\n\n0\n\n\n0.0055292\n\n\n13\n\n\n46.11873\n\n\n1\n\n\n0.0068944\n\n\n14\n\n\n50.32229\n\n\n0\n\n\n0.0092674\n\n\n15\n\n\n54.79747\n\n\n0\n\n\n0.0143167\n\n\n16\n\n\n49.44857\n\n\n0\n\n\n0.0085130\n\n\n17\n\n\n47.44495\n\n\n1\n\n\n0.0078428\n\n\n18\n\n\n45.44402\n\n\n0\n\n\n0.0057685\n\n\n19\n\n\n45.81414\n\n\n0\n\n\n0.0059797\n\n\n20\n\n\n62.07918\n\n\n0\n\n\n0.0290523\n\n\n21\n\n\n50.67044\n\n\n1\n\n\n0.0107303\n\n\n22\n\n\n47.54657\n\n\n1\n\n\n0.0079207\n\n\n23\n\n\n47.79726\n\n\n1\n\n\n0.0081160\n\n\n24\n\n\n52.29795\n\n\n0\n\n\n0.0112291\n\n\n25\n\n\n46.53140\n\n\n0\n\n\n0.0064114\n\n\n26\n\n\n42.75898\n\n\n0\n\n\n0.0044436\n\n\n27\n\n\n52.87378\n\n\n0\n\n\n0.0118754\n\n\n28\n\n\n44.88172\n\n\n1\n\n\n0.0061134\n\n\n29\n\n\n49.92431\n\n\n0\n\n\n0.0089158\n\n\n30\n\n\n45.32026\n\n\n1\n\n\n0.0063796\n\n\n31\n\n\n55.51149\n\n\n1\n\n\n0.0171766\n\n\n32\n\n\n47.62203\n\n\n0\n\n\n0.0071283\n\n\n33\n\n\n46.45280\n\n\n0\n\n\n0.0063627\n\n\n34\n\n\n47.49371\n\n\n0\n\n\n0.0070400\n\n\n35\n\n\n41.85453\n\n\n0\n\n\n0.0040697\n\n\n36\n\n\n44.16190\n\n\n1\n\n\n0.0057004\n\n\n37\n\n\n39.09980\n\n\n0\n\n\n0.0031138\n\n\n38\n\n\n43.29503\n\n\n1\n\n\n0.0052398\n\n\n39\n\n\n48.52853\n\n\n1\n\n\n0.0087138\n\n\n40\n\n\n47.67051\n\n\n1\n\n\n0.0080166\n\n\n41\n\n\n57.24748\n\n\n0\n\n\n0.0181656\n\n\n42\n\n\n44.65679\n\n\n1\n\n\n0.0059813\n\n\n43\n\n\n45.72318\n\n\n0\n\n\n0.0059271\n\n\n44\n\n\n48.59689\n\n\n0\n\n\n0.0078367\n\n\n45\n\n\n45.02830\n\n\n1\n\n\n0.0062012\n\n\n46\n\n\n45.15743\n\n\n0\n\n\n0.0056100\n\n\n47\n\n\n44.46341\n\n\n1\n\n\n0.0058699\n\n\n48\n\n\n43.74007\n\n\n0\n\n\n0.0048881\n\n\n49\n\n\n47.38086\n\n\n1\n\n\n0.0077941\n\n\n50\n\n\n47.51575\n\n\n1\n\n\n0.0078970\n\n\n51\n\n\n40.96984\n\n\n0\n\n\n0.0037344\n\n\n52\n\n\n47.08962\n\n\n0\n\n\n0.0067689\n\n\n53\n\n\n44.45555\n\n\n0\n\n\n0.0052401\n\n\n54\n\n\n44.92519\n\n\n0\n\n\n0.0054848\n\n\n55\n\n\n49.18845\n\n\n0\n\n\n0.0083005\n\n\n56\n\n\n52.81528\n\n\n0\n\n\n0.0118081\n\n\n57\n\n\n58.23909\n\n\n1\n\n\n0.0223904\n\n\n58\n\n\n46.13323\n\n\n1\n\n\n0.0069041\n\n\n59\n\n\n58.02955\n\n\n0\n\n\n0.0196001\n\n\n60\n\n\n44.21096\n\n\n0\n\n\n0.0051170\n\n\n61\n\n\n53.28294\n\n\n1\n\n\n0.0138317\n\n\n62\n\n\n62.74496\n\n\n1\n\n\n0.0346929\n\n\n63\n\n\n49.82620\n\n\n0\n\n\n0.0088312\n\n\n64\n\n\n46.65183\n\n\n1\n\n\n0.0072610\n\n\n65\n\n\n49.96198\n\n\n0\n\n\n0.0089485\n\n\n66\n\n\n58.88542\n\n\n1\n\n\n0.0238419\n\n\n67\n\n\n44.30696\n\n\n1\n\n\n0.0057813\n\n\n68\n\n\n56.83914\n\n\n1\n\n\n0.0195422\n\n\n69\n\n\n56.64782\n\n\n1\n\n\n0.0191822\n\n\n70\n\n\n51.68236\n\n\n0\n\n\n0.0105770\n\n\n71\n\n\n50.03446\n\n\n1\n\n\n0.0100871\n\n\n72\n\n\n47.72266\n\n\n1\n\n\n0.0080574\n\n\n73\n\n\n48.16738\n\n\n0\n\n\n0.0075163\n\n\n74\n\n\n53.24143\n\n\n1\n\n\n0.0137761\n\n\n75\n\n\n60.35135\n\n\n1\n\n\n0.0274924\n\n\n76\n\n\n49.23301\n\n\n0\n\n\n0.0083365\n\n\n77\n\n\n43.04650\n\n\n0\n\n\n0.0045695\n\n\n78\n\n\n46.38209\n\n\n1\n\n\n0.0070731\n\n\n79\n\n\n51.29131\n\n\n1\n\n\n0.0113977\n\n\n80\n\n\n48.41470\n\n\n0\n\n\n0.0076992\n\n\n81\n\n\n49.11105\n\n\n0\n\n\n0.0082383\n\n\n82\n\n\n49.15003\n\n\n1\n\n\n0.0092563\n\n\n83\n\n\n43.13849\n\n\n1\n\n\n0.0051607\n\n\n84\n\n\n49.13106\n\n\n0\n\n\n0.0082543\n\n\n85\n\n\n54.25116\n\n\n0\n\n\n0.0135764\n\n\n86\n\n\n53.48804\n\n\n0\n\n\n0.0126059\n\n\n87\n\n\n52.74999\n\n\n0\n\n\n0.0117334\n\n\n88\n\n\n47.98634\n\n\n0\n\n\n0.0073852\n\n\n89\n\n\n49.04203\n\n\n0\n\n\n0.0081832\n\n\n90\n\n\n44.02736\n\n\n1\n\n\n0.0056263\n\n\n91\n\n\n49.73421\n\n\n0\n\n\n0.0087526\n\n\n92\n\n\n51.27598\n\n\n1\n\n\n0.0113807\n\n\n93\n\n\n58.52982\n\n\n1\n\n\n0.0230320\n\n\n94\n\n\n55.00757\n\n\n0\n\n\n0.0146120\n\n\n95\n\n\n47.52208\n\n\n0\n\n\n0.0070594\n\n\n96\n\n\n51.77775\n\n\n1\n\n\n0.0119494\n\n\n97\n\n\n44.32696\n\n\n0\n\n\n0.0051750\n\n\n98\n\n\n54.39102\n\n\n0\n\n\n0.0137622\n\n\n99\n\n\n54.86458\n\n\n1\n\n\n0.0161300\n\n\n100\n\n\n60.60559\n\n\n0\n\n\n0.0251759\n\n\n\nUsing p to calculate weighted mean and proportion of age and gender,\n\n\nt(p)%*%matrix(c(age,gender),ncol=2)\n\n\n         [,1]      [,2]\n[1,] 52.00021 0.4999817\n\nWe also calculate effective sample size\n\n\nw = c(exp(Z%*%alpha))\ness = sum(w)^2/sum(w^2)\ness\n\n\n[1] 74.22902\n\nThus, to balance effect modifiers, we lose about 26 participants.\nFurther elucidations and applications can be found in Signorovitch’s paper and NICE technical support documents.1,2\n\n\n\n1. Signorovitch JE, Wu EQ, Yu AP, et al. Comparative effectiveness without head-to-head trials. Pharmacoeconomics 2010; 28: 935–945.\n\n\n2. Phillippo D, Ades T, Dias S, et al. NICE DSU technical support document 18: Methods for population-adjusted indirect comparisons in submissions to NICE.\n\n\n\n\n",
      "last_modified": "2022-03-19T18:07:41-04:00"
    },
    {
      "path": "z-some-terms-in-stats.html",
      "title": "Some Terms Used in Statistics",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nVariability\n\nLet us start with distinguishing three terms, namely variability, heterogeneity and uncertainty.\nVariability\nThe variability is often called first-order\n\n\n\n",
      "last_modified": "2022-03-19T18:07:43-04:00"
    },
    {
      "path": "z-statistical-cost-effectiveness.html",
      "title": "Statistical cost-effectiveness",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1 Mathematical background\n2 The setup of decision-making in health economics\n2.1 Statistical framework\n2.2 Decision process\n2.3 Choosing a utility function: The net benefit\n\n3 The value of information (VoI)\n3.1 Motivation\n3.2 Probabilistic sensitivity analysis (PSA)\n3.3 The value of information (VoI)\n3.4 The value of partial information\n\n4 Examples\n\n\n\nhide\n.makebox{\nborder: 2px solid #566573;\nborder-radius: 5px; \nmargin-bottom: 15px;\n}\n\n.makebox{\nborder: 2px solid #566573;\nborder-radius: 5px; \nmargin-bottom: 15px;\n}\n\n1 Mathematical background\nLet \\(t \\in \\mathcal{T}\\) be a decision or intervention/treatment, and \\(o \\in \\mathcal{O}\\) consequence or outcome. We also introduce the random quantity \\(\\boldsymbol{\\omega} \\in \\Omega\\), so the outcome can be described by relationship \\(o = (\\boldsymbol{\\omega},t)\\) which could be interpreted as the result of selecting treatment \\(t\\) with a series of random quantities obtained in the future.\nSince \\(\\boldsymbol{\\omega}\\) is random component, we have the probability measure \\(p(\\boldsymbol{\\omega})\\), i.e. \\(p: \\Omega \\rightarrow [0,1]\\). Also, the value of each outcome is quantified by measure of utility \\(u: \\mathcal{O} \\rightarrow \\mathbb{R}\\). In general, the random component is nothing but distribution, i.e. \\(\\boldsymbol{\\omega} = (y,\\boldsymbol{\\theta})\\) where \\(y \\in \\mathcal{Y}\\) is future result of individual(s), and \\(\\boldsymbol{\\theta} \\in \\mathbb{\\Theta}\\) is a set of parameters. Having had sub-components of \\(\\boldsymbol{\\omega}\\), the outcome can be rewritten as\n\\[\n\\begin{aligned}\n&o = (y,\\boldsymbol{\\theta},t), \\text{ implying that } \\\\\n&p(o) = u(y,\\boldsymbol{\\theta},t)\n\\end{aligned}\n\\] where\n\\[\np(y,\\boldsymbol{\\theta}) = p(y|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta}).\n\\]\nFor example, we shall consider 5 states of happiness denoted by \\(\\{y_i\\}_{i=1}^5\\), each state has probability \\(\\boldsymbol{\\theta}_i^t, t = 0,1\\). We temporarily assume that the degree of uncertainty of \\(\\boldsymbol{\\theta}^t_i\\) equal zero. Thus, The preferred outcome is can be obtained in either of two ways\ndirectly, base on the probability of first state of each treatment, i.e. \\(\\boldsymbol{\\theta}^t_1\\).\nindirectly, through other states. In fact, state \\(\\{y_{s}\\}_{s=2}^5\\) is often defined as the utilities equivalent to a scenario in which perfect health is obtained with probability \\(\\pi_s = u(y_s,t)\\).\nUsing the law of total probability we obtain the chance of preferred consequence \\(y_1\\) as follows\n\\[\n\\begin{aligned}\n\\mathbb{P}(y_1|\\boldsymbol{\\theta}^t) &= \\boldsymbol{\\theta}_1^t + \\sum_{s=2}^5\\pi_s\\mathbb{P}(y_s|\\boldsymbol{\\theta}^t)\n\\\\&= \\boldsymbol{\\theta}_1^t+ \\sum_{s=1}^5\\pi_s\\boldsymbol{\\theta}_s^t\n\\\\&=\\sum_{s=1}^5u(y_s,t)\\boldsymbol{\\theta}_s^t\n\\\\&= \\mathbb{E}[u(Y,t)]\n\\end{aligned}\n\\tag{1}\n\\]\nIn practice, \\(\\boldsymbol{\\theta}^t\\) always involves uncertainty, so we will need to consider \\(p(\\boldsymbol{\\theta}^t|\\mathcal{D})\\), where \\(\\mathcal{D}\\) is the background information. The implication of Eq.(1) is that the probability of obtaining the preferred options amounts to the expectation of utility function, more commonly referred to as the expected utility denoting \\(\\mathcal{U}^t\\). Thus, the optimal decision criterion is to maximize the expected utility, i.e.\n\n\\[\nt^* = \\arg\\max_{\\mathcal{T}}\\mathcal{U}^t\n\\]\n\nAlso, we can generalize Eq.(1) as follows\n\\[\n\\mathcal{U}^t = \\int_{\\Theta}\\int_{\\mathcal{Y}} u(y,t)p(y|\\boldsymbol{\\theta}^t)p(\\boldsymbol{\\theta}^t|\\mathcal{D})d\\color{red}{y}d\\boldsymbol{\\color{blue}{\\theta}}^t\n\\tag{2}\n\\]\n2 The setup of decision-making in health economics\n2.1 Statistical framework\nLet \\(t \\in \\mathcal{T}:= \\{0,1\\}\\) be a set of interventions and \\(y_i\\) outcome of \\(i^{th}\\) subject which is constituted by effectiveness and cost, i.e. \\(y = (e,c)\\). Also, \\(\\boldsymbol{\\theta} = (\\theta_0,\\theta_1)^{\\top}\\) is the vector of parameters of interest. First, we aim to find the conditional distribution of \\(\\boldsymbol{\\theta}\\) given data, i.e. posterior distribution of \\(\\boldsymbol{\\theta}\\). Let \\(\\mathcal{D}_t = \\{y_i\\}_{i=1}^{n_t}\\) and \\(\\mathcal{D} = \\bigcup_{t\\in\\mathcal{T}}\\mathcal{D}_t\\), so the posterior has the form \\(p(\\boldsymbol{\\theta}|\\mathcal{D})\\). In the second step, therefore, we are interested in predictive model which can be obtained by simulation.\nFor example, suppose that \\(\\pi_1\\) and \\(\\pi_0\\) are probabilities that a patient is treated by active treatment and placebo, respectively, and \\(\\pi_1 = \\pi_0\\rho\\) where \\(\\rho\\) is the measure of difference, i.e absolute or relative difference. Conditionally on \\(\\pi_t\\), \\(\\gamma\\) is the probability of needing ambulatory care and \\(1-\\gamma\\) is the probability a patient requires hospital admission.\nWe take the sample size of \\(N =1000\\) and the number of patients experiencing side effect is \\(SE_t \\sim \\mathcal{Bin}(\\pi_t,N)\\); the number of patients requiring ambulatory care is \\(A_t \\sim \\mathcal{Bin}(\\gamma,SE_t)\\), and hence the number of patients with side effects requiring hospital admission is \\(H_t = SE_t-A_t\\). Also, \\(c_t^{drug}\\) is a fixed cost value and \\(c^{hosp}\\) and \\(c^{amb}\\) are costs of ambulatory care and hospital admission, such two costs are independent. We now modify the parameters of interest as \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1)\\) where \\(\\boldsymbol{\\theta}_0 = (\\pi_0,\\gamma,c^{amb},c^{hosp})\\) and \\(\\boldsymbol{\\theta}_1 = (\\pi_1,\\gamma,c^{amb},c^{hosp})\\). Therefore, condition on \\(\\boldsymbol{\\theta}\\) the observable outcome is \\(Y = (SE_t,A_t,H_t)\\).\n2.2 Decision process\nSuppose an intervention \\(t\\) is applied and we obtain the outcome \\(y\\). Thus, we can quantify by combining a clinical effectiveness \\(e\\) of the outcome \\(y\\) and the costs \\(c\\) associated with intervention \\(t\\), for each \\((y,t)\\) we associate \\((e,c)\\). We now let\n\\[\n\\Delta_e := \\mathbb{E}(e|\\theta_1) - \\mathbb{E}(e|\\theta_0), \\text{ and}\\quad \\Delta_c:=\\mathbb{E}(c|\\theta_1) - \\mathbb{E}(c|\\theta_0)\n\\]\nare the increment in the mean effectiveness and the increment in the mean cost. Obviously, these two functions are the function of \\(\\boldsymbol{\\theta}\\). Recall that we aim to find \\(\\mathcal{U}^*:= \\max\\mathcal{U}_t\\), i.e. we choose \\(t=1\\) if\n\\[\nEIB:= \\mathcal{U}_1 - \\mathcal{U}_0 > 0\n\\tag{3}\n\\]\nthus,\n\\[\n\\mathcal{U}^* = \\max\\{EIB,0\\}+\\mathcal{U}_0.\n\\]\n2.3 Choosing a utility function: The net benefit\nTo obtain Eq.(2) we will need to a pre-defined function of utility \\(u(y,t)\\) through \\(e\\) and \\(c\\), the net benefit, for example,\n\\[\nu(y,t) = ke-c,\n\\]\nwhere \\(k\\) is a willingness-to-pay parameter used to put the cost and benefit on the same scale, and it is the budget the decision-maker is willing to pay to upgrade the benefits by one unit. Let us review the relationship among aforementioned measures:\n\nThe expected incremental benefit: \\(EIB:=\\mathcal{U}_t-\\mathcal{U}_0\\);\nThe expected utility \\(\\mathcal{U}_t = \\mathbb{E}[u(y,t)]\\);\n\\(u(y,t) = ke-c\\)\n\\(\\Delta_e:= \\mathbb{E}(e|\\theta_1)-\\mathbb{E}(e|\\theta_0)\\), and \\(\\Delta_c:= \\mathbb{E}(c|\\theta_1) - \\mathbb{E}(c|\\theta_0)\\)\n\nThus,\n\\[\nEIB = \\mathbb{E}(k\\Delta_e-\\Delta_c) = k\\mathbb{E}(\\Delta_e) - \\mathbb{E}(\\Delta_c),\n\\tag{4}\n\\]\nand because \\(\\Delta_e\\) and \\(\\Delta_c\\) are expectations of \\(e|\\boldsymbol{\\theta}\\) and \\(c|\\boldsymbol{\\theta}\\), by total of expectation, \\(EIB\\) is an expectation on distribution of \\(\\boldsymbol{\\theta}\\). Let us divide both sides of Eq.(4) and arrange terms we obtain\n\\[\nk = \\frac{EIB+\\mathbb{E}(\\Delta_c)}{\\mathbb{E}(\\Delta_e)} = \\frac{EIB}{\\mathbb{E}(\\Delta_e)}+ICER\n\\]\nthus,\n\\[\nICER = k-\\frac{EIB}{\\mathbb{E}(\\Delta_e)}\n\\]\nif \\(EIB >0\\) (by Eq.(3) this is the condition we expect to see), we have\n\\[\nICER < k,\n\\]\nso interventions having the \\(ICER\\) is less than the willing-to-pay threshold are deemed cost-effective.\nAlso, we will need to define an appropriate measures of the cost and effectiveness. The total cost relative to each treatment can be added up the cost of each patient, i.e\n\\[\nc_t := c_t^{drug}(N-SE_t)+ (c_t^{drug}+c^{amb})A_t+(c_t^{drug}+c^{hosp})H_t.\n\\tag{5}\n\\]\nAs for the measure of effectiveness, we count the number of patients who do not experience side effects\n\\[\ne_t:= N-SE_t.\n\\tag{6}\n\\]\n3 The value of information (VoI)\n3.1 Motivation\nWe shall consider the following table\n\n\nhide\n\ntab1<-\ntibble(decision = c(\"d1\",\"d2\",\"d3\",\"probability\"),\n       `state 1` = c(12,15,5,0.5),\n       `state 2` = c(9,11,18,0.2), \n       `state 3` = c(13,8,10,0.3)\n       )\nMyTable(tab1, \"current information.\")\n\n\n\nTable 1: current information.\n\n\ndecision\n\n\nstate 1\n\n\nstate 2\n\n\nstate 3\n\n\nd1\n\n\n12.0\n\n\n9.0\n\n\n13.0\n\n\nd2\n\n\n15.0\n\n\n11.0\n\n\n8.0\n\n\nd3\n\n\n5.0\n\n\n18.0\n\n\n10.0\n\n\nprobability\n\n\n0.5\n\n\n0.2\n\n\n0.3\n\n\nand two measures expected value with perfect information (EVwPI) and expected value without perfect information (EVwoPI), which are defined as follows\n\\[\nEVwPI = \\mathbb{E}[\\max(S)];\n\\tag{7}\n\\] \\[\nEVwoPI = \\max\\mathbb{E}(S).\n\\tag{8}\n\\]\nHence, we obtain \\(\\mathbb{E}(S)\\) for \\(d_1\\), \\(d_2\\) and \\(d_3\\) are\n\n\nhide\n\nexp_s<-\nsapply(tab1[-4,-1], function(x) as.matrix(tab1[4,-1])%*%x)\nexp_s\n\n\nstate 1 state 2 state 3 \n   10.5    12.1    11.1 \n\nso \\(\\max\\mathbb{E}(S)=\\) 12.1. Then we calculate \\(\\mathbb{E}[\\max(S)]\\)\n\n\nhide\n\nexp_max<-\nas.matrix(tab1[4,-1])%*%sapply(tab1[-4,-1], max)%>% c()\nexp_max\n\n\n[1] 15\n\nFinally, the so-called the expected value of perfect information (EVPI) is the difference between EVwPI and EVwoPI, i.e. \\(EVwoPI - EVwPI =\\) 2.9.\nThe rationale behind EVPI is very much similar to these calculations.\n3.2 Probabilistic sensitivity analysis (PSA)\nWe now suppose the prior distribution \\(p(\\boldsymbol{\\theta}|\\mathcal{D})\\) is close to one-point distribution at the true value. Hence, the expected utility is\n\\[\nU(\\theta_t) := \\mathbb{E}[u(Y,t)|\\mathcal{D}] = \\int u(y,t)p(y|\\theta_t)d\\color{red}{y}.\n\\]\nand \\(U^*(\\boldsymbol{\\theta}):= \\max_t U(\\theta_t)\\). Thus, we would choose \\(t=1\\) as the incremental benefit \\(IB(\\boldsymbol{\\theta}) > 0\\) where\n\\[\nIB(\\boldsymbol{\\theta}):= U(\\theta_1) - U(\\theta_0),\n\\]\nand as previous expansion, we are able to obtain \\(U^*(\\boldsymbol{\\theta}) = \\max\\{IB(\\boldsymbol{\\theta}),0\\}+U(\\theta_0)\\). In practice, the certainty of \\(\\boldsymbol{\\theta}\\) is not able to happen, so the value of information is often used rather than PSA.\n3.3 The value of information (VoI)\nVoI will be obtained by two utilities term, namely \\(U^*(\\boldsymbol{\\theta})\\) and \\(\\mathcal{U}^*\\) relative to the expectation value with perfect information (EVwPI) and the expectation value without perfect information (EVwoPI). Indeed,\n\\[\nVI(\\boldsymbol{\\theta}) := U^*(\\boldsymbol{\\theta}) - \\mathcal{U}^*,\n\\]\nwhich is a function of a random component \\(\\boldsymbol{\\theta}\\), and hence we are interested in the expectation of \\(VI\\) instead of itself, i.e the expected value of perfect information (EVPI)\n\\[\nEVPI := \\mathbb{E}[VI(\\boldsymbol{\\theta})] = \\mathbb{E}_{\\boldsymbol{\\theta}|\\mathcal{}D}[U^*(\\boldsymbol{\\theta})] -\\mathbb{E}_{\\boldsymbol{\\theta}|\\mathcal{D}}(\\mathcal{U}^*) = \\mathcal{V}^*-\\mathcal{U}^*,\n\\]\nwe can consider \\(\\mathcal{V}^*\\) as EVwPI or “expectation of maximum”, and \\(\\mathcal{U}^*\\) EVwoPI or “maximum of expectation”.\nEVPI can also be described through the opportunity loss (OL). Let \\(\\tau =\\arg\\max_t(\\mathcal{U}_t)\\), we have\n\\[\nOL(\\boldsymbol{\\theta}) := U^*(\\boldsymbol{\\theta}) - U(\\theta_{\\tau})\n\\]\nand then\n\\[\n\\mathbb{E}[OL(\\boldsymbol{\\theta})] = \\mathbb{E}_{\\boldsymbol{\\theta}|\\mathcal{D}}[U^*(\\boldsymbol{\\theta})] - \\mathbb{E}_{\\theta_{\\tau}|\\mathcal{D}}[U(\\theta_{\\tau})] = \\mathcal{V}^*-\\mathcal{U}^*\n\\]\nnote that \\(\\mathbb{E}_{\\theta_{\\tau}|\\mathcal{D}}[U(\\theta_{\\tau})]\\) is equivalent to \\(\\max_{\\tau}\\mathbb{E}_{\\boldsymbol{\\theta}|\\mathcal{D}}[U(\\boldsymbol{\\theta})]\\)\n3.4 The value of partial information\n4 Examples\nFor the sake of illustration, we shall consider the following example. Let \\(t \\in \\{0,1\\}\\) be treatment \\(t\\), \\(\\pi_t\\) the probability of an exposure under treatment \\(t\\). The effect size of interest is usually ratio or difference between probabilities \\(\\pi_0\\) and \\(\\pi_1\\), so we assume \\(\\pi_1 = \\pi_0\\rho\\) where \\(\\rho = \\pi_1/\\pi_0\\) is the relative difference between two treatments. Conditioning on \\(\\pi_t\\), \\(\\gamma\\) indicates the probability of needing ambulatory care. Therefore, the probability that an exposure needs hospital admission is \\((1-\\gamma)\\). Furthermore, let \\(N= 1000\\), so the total number of patients having exposure is \\(SE_t \\sim \\mathcal{B}in(N,\\pi_t)\\) and \\(A_t \\sim \\mathcal{B}in(\\gamma,SE_t)\\) is the number of exposures requiring ambulatory care. As a result, \\(H_t=SE_t-A_t\\) is the total number of exposures requiring hospital admission.\nAs for the costs, all patients are subjected to the cost \\(c^{drug}_t\\). Additionally, those exposed patients are required paying extra cost, namely \\(c^{amb}\\) for ambulatory care and \\(c^{hosp}\\) for hospital admission. Hence, we consider parameter \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1)\\) where \\(\\boldsymbol{\\theta}_t = (\\pi_t,\\gamma,c^{amb},c^{hosp})\\) with \\(t=0,1\\).\nWe now summary the model as follows:\nSuppose we have observed N.studies studies, each study reported the number of exposures se out of \\(n\\) patients and amb patients exposed requires only ambulatory care. This information would be used to update \\(\\pi_0\\) and \\(\\gamma\\). Thus, the prior information in this case can be expressed as \\(Y = (se_0,amb_0)\\). \\(t=0\\) is the current treatment being used.\nBased on the prior information we can can set \\(se_0 \\sim \\mathcal{B}in(\\pi_0,n)\\) and \\(amb_0 \\sim \\mathcal{B}in(\\gamma,se_0)\\);\nAlso,\n\\(\\pi_1 := \\pi_0\\rho\\), and\n\\(\\pi_0 \\sim \\mathcal{B}eta(a_{\\pi_0},b_{\\pi_0})\\), \\(\\rho \\sim \\mathcal{N}(m_{\\rho},\\tau_{\\rho})\\) and \\(\\gamma \\sim \\mathcal{B}eta(a_{\\gamma},b_{\\gamma})\\).\nFor the costs, we set \\(c^{amb} \\sim \\log\\mathcal{N}(m_{amb},\\tau_{amb})\\) and \\(c^{hosp} \\sim \\log\\mathcal{N}(m_{hosp},\\tau_{hosp})\\).\n\nFor prediction, we estimate the number of exposures under treatment \\(t =1\\), i.e \\(SE_1 \\sim \\mathcal{B}in(\\pi_1,N)\\), and the number of patients exposed requiring ambulatory \\(A_1 \\sim \\mathcal{B}in(\\gamma,SE_1)\\).\nBesides, we set \\(c_0^{drug} = 110\\) and \\(c_1^{drug} = 520\\).\nObviously, step (1) is likelihood that we can observed from data; step (2) is prior information obtained from previous studies; step (4) is predictive distribution of interest; and step (4) is pre-defined values.\nIn step 1, we shall generate fake data as follows\n\n\nhide\n\nset.seed(1234)\n## Generates data for the number of side effects under t=0\nN.studies <- 5 # the number of accessible studies \nsample.size <- 32 # potential sample size in each study \nprop.pi <- .24 # TRUE VALUE of the proportion of exposure\nn <- rpois(N.studies,sample.size) # we simulate sample size for N.studies studies. \nse <- rbinom(N.studies,n,prop.pi) # simulate the number of exposure for each study \n\n## Generates the data for the number of patients with exposure requiring ambulatory care\nprop.gamma <- .619 # TRUE VALUE of the proportion of patients exposed requiring ambulatory\namb <- rbinom(N.studies,se,prop.gamma) # simulate the number of ambulatory \n\ndat<- tibble(\n          Study = 1:5,\n          n = n,\n          exposure = se,\n          ambulatory = amb)\n\nMyTable(dat, \"Observed data\")\n\n\n\nTable 2: Observed data\n\n\nStudy\n\n\nn\n\n\nexposure\n\n\nambulatory\n\n\n1\n\n\n25\n\n\n5\n\n\n4\n\n\n2\n\n\n33\n\n\n11\n\n\n8\n\n\n3\n\n\n34\n\n\n7\n\n\n5\n\n\n4\n\n\n27\n\n\n9\n\n\n6\n\n\n5\n\n\n34\n\n\n7\n\n\n5\n\n\nIn step 2, we shall set hyper-parameters for prior distributions. There being 5 prior distributions, hyper-parameters should be chosen such that such the available prior information at hand can be delineated adequately through hyper-parameters. Thus,\nDistribution of \\(\\pi_0\\): describes the proportion of exposure and follows \\(\\mathcal{B}eta\\) distribution. Thus, such hyper-parameters are used to warrant uncertainties of parameters. for illustration, we select mean \\(m_{\\pi_0} = 0.5\\) and standard deviation \\(s_{\\pi_0} = \\sqrt{0.125}\\) for the \\(\\mathcal{B}eta\\) distribution.\nDistribution of \\(\\gamma\\): delineates the probability that patients exposed requires ambulatory .Again, we set \\(\\gamma\\) to follow \\(\\mathcal{B}eta\\) distribution with \\(m_{\\gamma} = 0.5\\) and \\(s_{\\gamma} = \\sqrt{0.125}\\)\nDistribution of \\(\\rho\\): delineates relative difference between two groups, and set \\(\\rho \\sim \\mathcal{N}(0.8,0.2^2)\\).\nWe shall consider prior distributions in step (2.iii), i.e. distribution costs\nBoth costs for ambulatory and hospital follow log-normal with mean and standard deviation \\((120,20)\\) and \\((5500,980)\\), respectively.\nAlso, we need to define functions that convert prior metrics to distribution’s parameters. More specifically, we obtain parameters for Beta and log-normal distribution. Thus\n\n\nhide\n\nbetaPar <- function(m,s){\na <- m*((m*(1-m)/s^2)-1)\nb <- (1-m)*((m*(1-m)/s^2)-1)\nlist(a=a,b=b)\n}\n\nlognPar <- function(m,s) {\ns2 <- s^2\nmulog <- log(m)-0.5*log(1+s2/m^2)\ns2log <- log(1+(s2/m^2))\nsigmalog <- sqrt(s2log)\nlist(mulog=mulog,sigmalog=sigmalog)\n}\n\n\n\nThen, we utilize above functions to obtain hyper-parameters as follows abut\n\n\nhide\n\n## Computes the hyper-parameters for the informative prior distributions \nm.pi <- .5 \ns.pi <- sqrt(.125)\na.pi <- betaPar(m.pi,s.pi)$a\nb.pi <- betaPar(m.pi,s.pi)$b\n\nm.gamma <- .5\ns.gamma <- sqrt(.125)\na.gamma <- betaPar(m.gamma,s.gamma)$a\nb.gamma <- betaPar(m.gamma,s.gamma)$b\n\nm.rho <- 0.8\ns.rho <- 0.2\ntau.rho <- 1/s.rho^2\n\nmu.amb <- 120\nsd.amb <- 20\nm.amb <- lognPar(mu.amb,sd.amb)$mulog\ns.amb <- lognPar(mu.amb,sd.amb)$sigmalog\ntau.amb <- 1/s.amb^2\n\nmu.hosp <- 5500\nsd.hosp <- 980\nm.hosp <- lognPar(mu.hosp,sd.hosp)$mulog\ns.hosp <- lognPar(mu.hosp,sd.hosp)$sigmalog\ntau.hosp <- 1/s.hosp^2\n\n\n\nFinally, in step (3) and (4), we need to set population size that we want to predict and the cost of drug in each group.\n\n\nhide\n\nN = 1000\nc.drug <- c(110,520)\n\n\n\nWe now define the model and run Bayesian\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n  \n  #likelihood\n  for(i in 1:N.studies){\n  se[i] ~ dbin(pi[1],n[i])\n  amb[i] ~ dbin(gamma,se[i])\n  }\n  \n  # prior\n  pi[1] ~ dbeta(a.pi,b.pi)\n  pi[2]<- pi[1]*rho\n  \n  rho ~ dnorm(m.rho, tau.rho)\n  gamma ~ dbeta(a.gamma,b.gamma)\n  c.amb ~ dlnorm(m.amb,tau.amb)\n  c.hosp ~ dlnorm(m.hosp,tau.hosp)\n  \n  # Predictive distributions on the clinical outcomes\n  for(i in 1:2){\n  SE[i] ~ dbin(pi[i],N)\n  A[i] ~ dbin(gamma,SE[i])\n  H[i] <- SE[i] - A[i]\n  }\n  }\")\n\n\n# data setup\ndat = list(\"a.pi\" = a.pi,\"b.pi\" = b.pi,\"a.gamma\" = a.gamma,\"b.gamma\" = b.gamma,\n           \"m.amb\" = m.amb,\"tau.amb\" = tau.amb,\n           \"m.hosp\" = m.hosp,\"tau.hosp\" = tau.hosp, \"m.rho\" = m.rho,\n           \"tau.rho\" = tau.rho,\"se\" = se,\"amb\" = amb,\n           \"N.studies\" = N.studies, \"n\" = n, N = N)\n\n# initial stochastic variables\ninits <- function(){              \n  SE=rbinom(2,N,.2)\n  rho=runif(1)\n\n  list(\"pi\"=c(runif(1),NA),gamma=runif(1),c.amb=rlnorm(1),\n  c.hosp=rlnorm(1),rho = rho,SE=SE,A=rbinom(2,SE,.2)\n  )\n}\n\n# fitting model and burn-in 10000 iterations\nmodel <- jags.model(model_string, data =  dat, inits = inits(), n.chains = 2)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 10\n   Unobserved stochastic nodes: 9\n   Total graph size: 39\n\nInitializing model\n\nhide\n\nupdate(model, 10000, progress.bar=\"none\")\n\n# parameters of interest\nparams <- c(\"pi\",\"gamma\",\"c.amb\",\"c.hosp\",\"rho\",\"SE\",\"A\",\"H\")\n# get samples\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=500, progress.bar=\"none\")\n\n\n\nCost-effectiveness analysis\ncost and effectiveness is calculated using Eq.(6) and Eq.(5). Here we shall address the cost effectiveness by two manners, using function bcea of the package BCEA and coding manually. Then, we compare both results to ensure the later result aligns with the former.\n\n\nhide\n\namb = lift_dl(rbind)(samples)\nef = N - amb[,c(\"A[1]\",\"A[2]\")]\ncost = sapply(1:2, \\(i){\n     c.drug[i]*ef[,i]+(c.drug[i]+amb[,'c.amb'])*amb[,i]+(c.drug[i]+amb[,'c.hosp'])*amb[,i+2]\n})\n\nlibrary(BCEA)\ntreats <- c(\"Old Chemotherapy\",\"New Chemotherapy\")\nm <- bcea(e=ef,c=cost,ref=2,interventions=treats,Kmax=50000)\nsummary(m)\n\n\n\n\n(cont.)\n\n\n\n",
      "last_modified": "2022-03-19T18:07:51-04:00"
    },
    {
      "path": "z-the-power.html",
      "title": "Power and Type I Error",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nPrior group\nPredictive group\n\nFour types of measures pertaining to the power of hypothesis testing are posterior predictive power (PoPP), posterior conditional power (PoCP), prior predictive power (PrPP) and prior conditional power (PrCP). Such measures could be categorized into 2 groups characterized by the fact that whether or not the knowledge of current data is integrated.\nPrior group\nBoth PrPP and PrCP belong to this group since we consider variability of parameters through prior distribution. Let \\(\\boldsymbol{x}_1\\) and \\(\\boldsymbol{x}_2\\) are data generated in phase I and phase II, respectively. to obtain PrCP we use the prior information of parameters as fixed metrics, say \\(\\bar\\theta\\), and we generate full data \\(\\boldsymbol{x}\\) based on those parameters, i.e \\(p(\\boldsymbol{x}|\\bar\\theta)\\), where \\(\\boldsymbol{x}^{\\top} = (\\boldsymbol{x}_1^{\\top},\\boldsymbol{x}_2^{\\top})\\). Then, \\(\\boldsymbol{x}_1\\) will be used for analyses in the first phase, for example, futility analysis, superiority analysis based on pre-specified rule. All treatments that do not satisfy early dropping rule will be assessed in the second phase with the total sample size, i.e full data \\(\\boldsymbol{x}\\). Thus, The PrCP is calculated over \\(S\\) simulated data \\(\\boldsymbol{x}\\). For example, we simulate \\(S\\) full data set \\(\\{\\boldsymbol{x}^{(1)},\\boldsymbol{x}^{(2)},\\dots,\\boldsymbol{x}^{(S)}\\}\\), for the \\(s\\)th set we use \\(\\boldsymbol{x}^{(s)}_1\\) for futility analysis, i.e\n\\[\np^{(s)} := \\mathbb{P}(\\theta_t<\\theta_0|\\boldsymbol{x}_1^{(s)})\n\\] where \\(t = 1,2\\), \\(\\theta_0\\) is a fixed value. The treatment \\(t\\) will be dropped early if \\(p^{(s)}\\ge \\delta\\), where \\(\\delta\\) is a threshold representing the minimum value \\(\\mathbb{P}(\\theta_t < \\theta_0|\\boldsymbol{x}_1)\\) is significant. Assuming that the first treatment is futile while the remaining treatments are considered effective and retained to be re-evaluated in the final analysis. In phase II, we evaluate efficacy of the remaining treatment based on full data \\(\\boldsymbol{x}\\), and PrCP is obtained as follows:\n\\[\n\\text{PrCP} = \\int_{\\mathfrak{X}} \\mathcal{I}[\\mathbb{P}(\\theta_2 >\\theta_0|\\boldsymbol{x})>\\delta]\\times f(\\boldsymbol{x}|\\bar\\theta)d\\boldsymbol{x},\n\\tag{1}\n\\] more specifically,\n\\[\n\\text{PrCP} = \\frac{1}{S}\\sum \\mathcal{I}[\\mathbb{P}(\\theta_2 >\\theta_0|\\boldsymbol{x})>\\delta],\n\\tag{2}\n\\]\nWe call the above expression prior conditional power since when simulating \\(S\\) data sets, we use one value \\(\\bar\\theta\\), which means our belief in \\(\\theta\\) is perfect, i.e \\(\\mathbb{P}(\\theta = \\bar\\theta) \\to 1\\).\nOn the other hand, when we integrate the variability of \\(\\theta\\) to calculate the prior power, it is call the PrPP. This can be achieved by adjusting (1) as follows:\n\\[\n\\text{PrPP} = \\int_{\\mathfrak{X}} \\int_{\\Theta} \\mathcal{I}[\\mathbb{P}(\\theta_2 >\\theta_0|\\boldsymbol{x})>\\delta]\\times f(\\boldsymbol{x}|\\theta)\\times f(\\theta)d\\theta d\\boldsymbol{x}.\n\\tag{3}\n\\]\nWe can obtain equation (3) by simulating. To this end, for the \\(s\\)th set (1) we simulate \\(\\theta\\) from prior distribution\\(f(\\theta|\\bar\\theta)\\), (2) and then we use such a \\(\\theta\\) to simulate \\(x\\). Both steps are repeated \\(n\\) times to get a set \\(\\boldsymbol{x}^{(s)}\\). Finally, PrPP will be calculated based on (2).\nPredictive group\nThis group differ the previous one from integrating information of data set \\(\\boldsymbol{x}_1\\) with prior information to obtain the posterior distribution that will be used to generate \\(\\boldsymbol{x}_2\\). PoPP will be calculated based on predictive distribution \\(f(\\boldsymbol{x}_2|\\boldsymbol{x}_1)\\), i.e.\n\\[\n\\begin{aligned}\n\\text{PoPP} &= \\int_{\\mathfrak{X}_2|\\mathfrak{X}_1}\\mathcal{I}[\\mathbb{P}(\\theta_2>\\theta_0|\\boldsymbol{x}_2,\\boldsymbol{x}_1)\\ge \\delta]\\times f(\\boldsymbol{x}_2|\\boldsymbol{x}_1)d(\\boldsymbol{x}_2)\n\\\\&= \\int_{\\mathfrak{X}_2|\\mathfrak{X}_1}\\int_{\\Theta} \\mathcal{I}[\\mathbb{P}(\\theta_2>\\theta_0|\\boldsymbol{x}_2,\\boldsymbol{x}_1)\\ge \\delta]\\times f(\\boldsymbol{x}_2|\\theta)\\times f(\\theta|\\boldsymbol{x}_1)d\\theta d\\boldsymbol{x}_2\n\\end{aligned}\n\\tag{4}\n\\]\nOn the other hand, if we ignore the variability of \\(f(\\theta|\\boldsymbol{x}_1)\\), we will obtain the PoCP, i.e.\n\\[\n\\text{PoPP} = \\int_{\\mathfrak{X}_2|\\mathfrak{X}_1} \\mathcal{I}[\\mathbb{P}(\\theta_2>\\theta_0|\\boldsymbol{x}_2,\\boldsymbol{x}_1)\\ge \\delta]\\times f(\\boldsymbol{x}_2|\\bar\\theta)d\\boldsymbol{x}_2,\n\\] where \\(\\bar\\theta\\) is defined such that \\(\\mathbb{P}(\\theta=\\bar\\theta|\\boldsymbol{x}_1)\\to 1\\), i.e. our belief in \\(\\bar\\theta\\) based on prior and interim (phase I) information is “unshakable”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2022-03-19T18:07:53-04:00"
    },
    {
      "path": "z-use-historical-data.html",
      "title": "Use Historical Data",
      "description": "*this note is*",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nMotivations\nMethods\nAn illustrative example\nNormal-normal hierachical model (NNHM)\n\n\nMotivations\nMethods\nBefore delving into the exposition, we shall mathematically notate information used in later analyses. Assuming that there are J trials intrinsic to historical information \\(\\mathcal{H}\\), more specifically this information is often summarized through summary statistics, of which three typical metrics comprise mean, standard deviation (SD), proportions. Let \\(\\boldsymbol{Y}_{\\mathcal{J}} = \\{Y_1,\\dots,Y_J\\}\\) is a collection of \\(\\mathcal{H}\\), and \\(\\theta_j\\) is parameter pertaining to \\(Y_j\\). We’d like to combine accrued data \\(\\boldsymbol{Y}_{\\mathcal{J}}\\) with current data \\(Y_*\\) to estimate \\(\\theta_*\\). \\(Y_*\\) is also called the direct evidence and \\(\\boldsymbol{Y}_{\\mathcal{J}}\\) indirect evidence. There appears two ways to achieve this\nObtain \\(p(\\theta_*|\\boldsymbol{Y}_{\\mathcal{J}})\\), and then update \\(\\theta_*\\) by \\(Y_*\\). This approach is called meta-analytic-predictive (MAP);\nDirectly obtain \\(p(\\theta_*|\\boldsymbol{Y}_{\\mathcal{J}},Y_*)\\), which is called meta-analytic-combined (MAC).\nAn illustrative example\nWe shall utilize the following example as an illustration for theories developed throughout this note. We found 5 studies that reported the time to independent walking (in days) of children with Guillain-Barre syndrome (GBS).1\n\n\nhide\n\ngbs = dplyr::tibble(\n     study = 1:5,\n     n = c(56,37,14,18,9),\n     median = c(40,NA,NA,43,NA),\n     mean = c(45.6,58.7,60.2,NA,50),\n     sd = c(24.7,44,43.6,NA,29)\n)\n\ngbs|>\n     {\\(x) modify(x, ~ifelse(is.na(.),\"\",.))}()|>\n     MyTable(\"*Historical data of the time to independent walking (in days) of children with Guillain-Barre syndrome.*\")\n\n\n\nTable 1: Historical data of the time to independent walking (in days) of children with Guillain-Barre syndrome.\n\n\nstudy\n\n\nn\n\n\nmedian\n\n\nmean\n\n\nsd\n\n\n1\n\n\n56\n\n\n40\n\n\n45.6\n\n\n24.7\n\n\n2\n\n\n37\n\n\n\n\n58.7\n\n\n44\n\n\n3\n\n\n14\n\n\n\n\n60.2\n\n\n43.6\n\n\n4\n\n\n18\n\n\n43\n\n\n\n\n\n\n5\n\n\n9\n\n\n\n\n50\n\n\n29\n\n\nNormal-normal hierachical model (NNHM)\nSuppose that \\(Y_j|\\theta_j \\sim \\mathcal{N}(\\theta_j,s^2_j)\\) which is called data model, and\n\\[\n\\theta_j = \\mu + \\varepsilon_j, \\quad \\varepsilon_j \\sim \\mathcal{N}(0,\\tau^2),\n\\tag{1}\n\\]\nor \\(\\theta_j|\\mu,\\tau \\sim \\mathcal{N}(\\mu,\\tau^2)\\) is parameter model. According to Table 1 the mean is consistently greater than SD through 5 studies, so parameter \\(\\mu\\) could follow log-normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). Therefore, the median, mean and variance of \\(\\mu\\) are \\(e^{\\mu}\\), \\(e^{\\mu+\\sigma^2/2}\\) and \\(e^{2\\mu+\\sigma^2}(e^{\\sigma^2}-1)\\), respectively. For each study in Table 1, we can calculate \\(\\mu\\) and \\(\\sigma\\) by method of moment. For study 2,3 and 5 we use the mean and SD while the median will be used for calculation for study 1 and 4. Formulae to calculate \\(\\sigma\\) and \\(\\mu\\) are\n\\[\n\\mu = \\ln(SD^2/mean^2+1), \\quad \\sigma = \\sqrt{2[\\ln(mean)-\\mu]},\n\\]\nrespectively. If median is used in lieu of mean,\n\n\n\n1. Goodmana SN, Sladkyb JT. A bayesian approach to randomized controlled trials in children utilizing information from adults: The case of guillain-barre syndrome. Clinical Trials 2005; 2: 305–310.\n\n\n\n\n",
      "last_modified": "2022-03-19T18:07:56-04:00"
    }
  ],
  "collections": []
}
