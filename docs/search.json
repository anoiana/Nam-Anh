{
  "articles": [
    {
      "path": "index.html",
      "title": "Bayesian Statistics",
      "description": "\"Begin with an estimate of the probability that any claim, belief, hypothesis is true, then look at any new data and update the probability given the new data.\"\n\n-Steven Novella-\n",
      "author": [],
      "contents": "\n\n“No mud, no lotus”\n\n\n“Smile to the cloud in your tea”\n\n\nThich Nhat Hanh\n\n\n\n\n",
      "last_modified": "2022-01-23T02:19:49-05:00"
    },
    {
      "path": "z-generate-sample-with-rjags.html",
      "title": "Generate Samples with `Rjags`",
      "description": "This note summarizes methods to generate samples and calculate measures of interest through such generated samples.",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1 Sampling with rjags\n1.1 Generalized t distribution\n1.2 Transformation\n1.3 A more complicated problem\n\n2 Prediction with unknown parameters\n\n1 Sampling with rjags\n1.1 Generalized t distribution\nSuppose one wants to generate samples from generalized \\(t\\) distribution whose density function is as follows\n\\[\\begin{equation}\nf(x) = \\frac{c\\sigma^{-1}}{\\bigg[n+\\Big(\\frac{x-\\theta}{\\sigma}\\Big)^2\\bigg]^{0.5(n+1)}},\n\\end{equation}\\]\none says \\(X \\sim t(\\theta,\\sigma,n)\\), where \\(\\theta, \\sigma\\) and \\(n\\) are location, scale parameter and degree of freedom.\nTo generate samples of \\(X\\), one can use rt function in R to simulate from standard \\(t\\) distribution and then transform such samples. one, however, now wants to simulate generalized \\(t\\) distribution directly, which does not require an extra step of transformation. To this end, one employs rjags which is popular for obtaining samples from posterior distribution in Bayesian framework.\nIt is worth noting that the generalized \\(t\\) distribution function dt in rjags requires mean and precision rather than location and scale as its parameters. For example, one simulates samples from a generalized \\(t\\) distribution whose mean and variance are 10 and 0.5, one shall need to plug in values of 10 and 1/0.5 as its first two parameters (third parameter is degree of freedom which equals 4).\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n Y ~ dt(10,2,4)\n}\")\nmodel <- jags.model(model_string, n.chains = 1, quiet = T)\n#update(model, 1000, progress.bar=\"none\")\nparams <- c(\"Y\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=1000, progress.bar=\"none\")\n\n##################\nx = samples[[1]][,1] %>% sort()\n\n\n\nLet us compare with samples obtained by R function\n\n\nhide\n\ncurve(wiqid::dt3(x,10, sqrt(0.5),4), xlim = c(0,20), lty = 3,, ylab = \"density\")\npoints(x,wiqid::dt3(x,10,sqrt(0.5),4), col = \"red\", type = \"l\", lty = 2)\n\n\n\n\n1.2 Transformation\nOne sometimes requires simulating samples from a transformed random variable, i.e one generates \\(Y = (2Z+1)^3\\), where \\(Z \\sim \\mathcal{N}(0,1)\\) and calculate \\(\\mathbb{E}(Y)\\) and \\(\\mathbb{P}(Y \\ge10)\\). Such measures can be obtained analytically, more specifically \\(\\mathbb{E}(Y) = 13\\) and \\(\\mathbb{P}(Y \\ge 10) = 0.28\\). [1]\nOne now uses rjags to simulate samples\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n Z ~ dnorm(0,1)\n Y <- pow(2*Z+1,3)\n P<- step(Y-10)\n}\")\nmodel <- jags.model(model_string, n.chains = 1, quiet = T)\nupdate(model, 1000, progress.bar=\"none\")\nparams <- c(\"Y\",\"P\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=2000, progress.bar=\"none\")\n\n##################\n\nexp_val = mean(samples[[1]][,2])\nprob = mean(samples[[1]][,1])\n\n\n\nand one obtained \\(\\mathbb{E}(Y) =\\) 13.1049 and \\(\\mathbb{P}(Y\\ge 10) =\\) 0.286.\n1.3 A more complicated problem\nSuppose one is required calculating how many items can be fixed with $1000 given the cost to repair an item follows gamma distribution with mean \\(\\mu = 100\\) and standard deviation \\(\\sigma = 50\\), i.e\n\\[\\begin{equation}\nX \\sim \\mathcal{G}am(4, 0.04)\n\\end{equation}\\]\none implements as follows\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n  for(i in 1:n){Y[i] ~ dgamma(4,0.04)}\n  sim[1]<- Y[1]\n  for(i in 2:n){sim[i] <- sim[i-1]+Y[i]}\n  for(i in 1:n){order[i]<- step(1000-sim[i])}\n  item<- sum(order[])\n  check <- step(sim[n]-1000)\n  \n}\")\ndata = list(n=20)\nmodel <- jags.model(model_string, n.chains = 1, data = data, quiet = T)\nupdate(model, 1000, progress.bar=\"none\")\nparams <- c(\"item\",\"check\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=10000, progress.bar=\"none\")\n\n##################\n\nitem = samples[[1]][,2] %>% mean()\ncheck = samples[[1]][,1] %>% mean()\n\n\n\nMean of item is 9.6279 indicates there are about 10 items which can be repaired, Mean of check equals 1 indicates \\(n=20\\) is sufficient.\n2 Prediction with unknown parameters\nfor the sake of ease, one shall consider a mixed distribution as follows\n\\[\\begin{equation}\nX = K\\mathcal{N}(0,1) + (1-K)\\mathcal{N}(5,1),\n\\end{equation}\\]\nwhere \\(K \\sim \\mathcal{B}er(0.3)\\). Alternatively, above distribution can be rewritten as follows\n\\[\\begin{equation}\nf(x) = \\int f(y|k)f(k)dk,\n\\tag{1}\n\\end{equation}\\]\nwhich is predictive prior distribution, where \\(k\\) was defined above, and \\(Y|K=1 \\sim \\mathcal{N}(0,1)\\) while \\(Y|K=0 \\sim \\mathcal{N}(5,1)\\). To simulate samples of above distribution, one employs Bernoulli and Normal distributions as follows\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\nalpha ~ dbin(0.3,1)\nx1 ~ dnorm(0,1)\nx2 ~ dnorm(5,1)\n# x = alpha*x1 + (1-alpha)*x2\nx = ifelse(alpha==1,x1,x2)\n}\")\n\ndata = list(n=20)\nmodel <- jags.model(model_string, n.chains = 1, quiet = T)\nupdate(model, 1000, progress.bar=\"none\")\nparams <- c(\"x\",\"alpha\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=10000, progress.bar=\"none\")\n\nsamples[[1]][,2] %>% hist()\n\n\n\n(1) can be expended on distribution of \\(K\\) that is known as prior distribution. For example, one wants to estimate, of 20 new students, how many ones will join painting club given the number of people joining the painting club follows Binomial distribution whose second paprameter follows Beta distribution. i.e\n\\[\\begin{equation}\nX \\sim \\mathcal{B}in(20,\\pi), \\quad \\pi \\sim \\mathcal{B}eta(3,2).\n\\end{equation}\\]\nThus, in rjags code one has\n\n\nhide\n\nmodel_string <- textConnection(\n  \"model{\n  p ~ dbeta(3,10)\n  x ~ dbin(p,20)}\"\n  \n  )\n\nmodel <- jags.model(model_string, n.chains = 1, quiet = T)\nupdate(model, 1000, progress.bar=\"none\")\nparams <- c(\"x\")\nsamples <- coda.samples(model,\n                        variable.names=params,\n                        n.iter=10000, progress.bar=\"none\")\n\nsummary(samples)\n\n\n\nIterations = 1001:11000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       4.52920        2.88631        0.02886        0.02886 \n\n2. Quantiles for each variable:\n\n 2.5%   25%   50%   75% 97.5% \n    0     2     4     6    11 \n\nhence, 5 of 20 new students will join the club. If one wants to calculate probability that there is at least 2 students who will join the club, the result is\n\n\nhide\n\nmean(samples[[1]][,1]>=5)\n\n\n[1] 0.4539\n\n\n\n\n[1] Fouley J-L. The BUGS book: A practical introduction to bayesian analysis 2013.\n\n\n\n\n",
      "last_modified": "2022-01-23T02:20:03-05:00"
    },
    {
      "path": "z-limit-of-sup-inf.html",
      "title": "Limit of Supremum and Infimum",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1 \\(\\pmb{\\lim\\sup}\\) and \\(\\pmb{\\lim\\inf}\\) of a sequence of real numbers\n1.1 \\(\\lim\\inf\\)\n1.2 \\(\\lim\\sup\\)\n\n2 \\(\\lim\\sup\\) and \\(\\lim\\inf\\) of a sequence of sets\n\n1 \\(\\pmb{\\lim\\sup}\\) and \\(\\pmb{\\lim\\inf}\\) of a sequence of real numbers\nSuppose we have a bounded sequence of real numbers \\((c_n)^{\\infty}_{n=1}\\). Also define \\[\n\\begin{aligned}\na_n &:= \\inf\\{c_k: k \\ge n\\}, \\\\\nb_n &:= \\sup\\{c_k:k \\ge n\\}.\n\\end{aligned}\n\\] We deem two cases that likely occur with \\((c_n)_{n=1}^{\\infty}\\)\n1.1 \\(\\lim\\inf\\)\nIf \\((a_n)\\) is bounded and increasing, then it has a limit called \\(a\\). We can define the so-called \\(\\lim\\inf\\) of such a sequence as follows \\[\n\\lim_{n}\\inf(c_n) = \\lim_{n \\to \\infty}\\inf\\{c_k: k \\ge n\\} \n\\]\nThe following figure illustrates for \\(\\lim\\inf\\):\n\n\nhide\n\nf = Vectorize(function(n) (-1)^n*(n+5)/n)\nn = 1:30\ns = f(n)\nd = cbind(n,s)\nset.seed(200)\nr = runif(100,-1,0)\nplot(d[d[,2]<0,], pch = 16, ylim = c(-6,0), cex = 1.2, ylab = \"sequence (c)\",type = \"o\")\npoints(r, pch = 16, col = \"red\")\nabline(h = -1, col = \"red\")\n\n\n\n\nFigure 1: example for \\(\\lim\\inf\\)\n\n\n\nas \\(n\\) goes to \\(\\infty\\), \\((c_n)\\) approaches to \\(-1\\) (never reach \\(-1\\)). The term inf or infimum is also called the greatest lower bound, this is, the greatest point of lower bound set. We can see that all black filled circle are less than \\(-1\\), so all of them can be deemed lower bound of the collection of red filled circles, and a point corresponding \\(n\\) as \\(n \\to \\infty\\) is the greatest point. By definition, this is called infimum or the greatest lower bound.\n1.2 \\(\\lim\\sup\\)\nSimilarly,\nIf \\((b_n)\\) is bounded and decreasing, it has a limit called \\(b\\). We can define the so-called \\(\\lim\\sup\\) of such a sequence as follows \\[\n\\lim_n\\sup(c_n) = \\lim_{n \\to \\infty}\\sup\\{c_k: k \\ge n\\}.\n\\]\nThe following figure illustrates for sup or supremum\n\n\nhide\n\nset.seed(200)\nr = runif(100,0,1)\nplot(d[d[,2]>0,], pch = 16, cex = 1.2, ylab = \"sequence (c)\", ylim = c(0,3.5),type = \"o\")\npoints(r, pch = 16, col = \"red\")\nabline(h = 1, col = \"red\")\n\n\n\n\nFigure 2: example for \\(\\lim\\sup\\)\n\n\n\nIn this case, all black filled circles are greater than \\(1\\), the sequence goes to \\(1\\) as \\(n\\) approaches to \\(\\infty\\) (never reach \\(1\\)). Thus, they are upper bounds of the collection of red filled circles. Also, a point corresponding \\(n\\) as \\(n \\to \\infty\\) is the least point of the upper bound set. This is called supremum or the least upper bound.\nIf we combine both figures, we obtain\n\n\nhide\n\nset.seed(200)\nr = runif(500,-1,1)\nplot(d[d[,2]<0,], pch = 16, ylim = c(-6,6), cex = 0.7, ylab = \"sequence (c)\", type = \"o\")\npoints(d[d[,2]>0,], pch = 16, cex = 0.7, ylab = \"sequence (c)\", ylim = c(0,3.5), type = \"o\")\npoints(r, pch = 16, col = \"red\")\nabline(h = c(-1,1), col = \"red\")\n\n\n\n\nFigure 3: example for \\(\\lim\\sup\\)\n\n\n\nTwo horizontal red lines represent for \\(a_n\\) and \\(b_n\\), and when \\(n\\) goes to \\(\\infty\\) they approach to \\(-1\\) and \\(1\\), respectively. If we extend the interval \\([-1,1]\\) to a general interval \\([a,b]\\), then \\([a_{n+1},b_{n+1}]\\) will be contained in \\([a_n,b_n]\\) for all \\(n\\). We can say \\(\\lim_{n \\to \\infty}[a_n,b_n] = [-1,1]\\). This is, as \\(n \\to \\infty\\) it approaches to the shortest length interval of \\([a_n,b_n]_{n=1}^{\\infty}\\).\nWe can also express the above idea by intersection, ie., the intersection of all intervals \\([a_n,b_n]_{n = 1,2,\\dots}\\) is \\([a,b] \\equiv [-1,1]\\). In other words, intersection can be utilized to express the “smallest” magnitude of length, area, volume, etc. This idea can be extended to limits of sets.\n2 \\(\\lim\\sup\\) and \\(\\lim\\inf\\) of a sequence of sets\nLet \\(A_n \\subset \\Omega\\), we define\n\\[\n\\inf_{k \\ge n}A_k := \\bigcap_{k=n}^{\\infty}A_k; \\quad \\sup_{k \\ge n}A_k := \\bigcup_{k=n}^{\\infty}A_k\n\\tag{1}\n\\]\nthus,\n\\[\\begin{align}\n&\\lim_{n \\to \\infty}\\inf A_n = \\bigcup_{n=1}^{\\infty}\\bigcap_{k=n}^{\\infty}A_k, \\\\\n&\\lim_{n \\to \\infty}\\sup A_n = \\bigcap_{n=1}^{\\infty}\\bigcup_{k=n}^{\\infty}A_k\n\\tag{2}\n\\end{align}\\]\nTo get the intuition about \\(\\lim\\sup\\) and \\(\\lim\\inf\\) in term of union and intersection, let us consider Eq.(1) in details. We will consider the case of intersection, second case can be explained in the same way.\nSuppose \\(n= 10\\),\n\\[\n\\begin{aligned}\n&B_1 = \\bigcap_{i=1}^{10}A_i \\\\\n&B_2 = \\bigcap_{i=2}^{10}A_i \\\\\n&\\quad \\dots \\\\\n&B_9 = A_9 \\cap A_{10} \\\\\n&B_{10} = A_{10}\n\\end{aligned}\n\\]\nhence we can conclude\n\\[\nB_1 \\le B_2 \\le \\dots \\le B_{10}\n\\]\nand hence \\(B_n\\) is an increasing sequence, so it has a limit. What we need is to find a set that corresponds to greatest lower bound. It is NOT akin to the sequence of real numbers, The sequence of sets i.e., the magnitude of sets is determined by the intersection and union of all available sets. Thus, to find the greatest set, we can’t only compare among \\(\\{B_i\\}_{i=1}^{10}\\), but we also compare all intersections and unions among \\(\\{B_i\\}_{i=1}^{10}\\). To this end, the greatest set is\n\\[\n\\bigcup_{i=1}^{10}B_i = \\bigcup_{n=1}^{10}\\bigcap_{i=n}^{10}A_i,\n\\]\nwhich has a general form in Eq.(2). The case of supremum can be explained with an analogous manner. To this end, we have\n\\[\n\\begin{aligned}\n&B_1 = \\bigcup_{i=1}^{10}A_i \\\\\n&B_2 = \\bigcup_{i=2}^{10}A_i \\\\\n&\\quad \\dots \\\\\n&B_9 = A_9 \\cup A_{10} \\\\\n&B_{10} = A_{10}\n\\end{aligned}\n\\] and then \\[\nB_1 \\ge B_2 \\ge \\dots \\ge B_{10}\n\\]\nhence \\(B_n\\) is an decreasing sequence and it has a limit. With an analogous manner, we can obtain the smallest set that is \\[\n\\bigcap_{i=1}^{10}B_i = \\bigcap_{n=1}^{10}\\bigcup_{i=n}^{10}A_i.\n\\] which is second equation in Eq.(2).\n\n\n\n",
      "last_modified": "2022-01-23T02:20:07-05:00"
    },
    {
      "path": "z-proof-of-maic.html",
      "title": "The Proof of MAIC",
      "author": [
        {
          "name": "Nam-Anh",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nLet \\(\\boldsymbol{p} = {(p_1,p_2,\\dots,p_n)}^{\\top}\\) be propensity score and \\(\\boldsymbol{X} = {(\\boldsymbol{x}_1,\\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_n)}^{\\top}\\) baseline characteristics of subjects in IPD. Also, \\(\\boldsymbol{\\mu}\\) denotes average vector of baseline characteristics found in SLD. We aim to estimate \\(\\boldsymbol{p}\\) such that \\({\\boldsymbol{p}}^{\\top}\\boldsymbol{X} = {\\boldsymbol{\\mu}}^{\\top}\\), i.e. we want to minimize Kullback Leibler divergence of \\(\\boldsymbol{p}\\) and empirical function with some constraints. Specifically, we want to minimize\n\\[\n-{\\boldsymbol{p}}^{\\top}\\ln(\\frac{1}{n\\boldsymbol{p}}),\n\\]\nwith two constraints:\n\\[\n{\\boldsymbol{p}}^{\\top}\\boldsymbol{1} = 1, \\quad and \\quad {\\boldsymbol{p}}^{\\top}\\boldsymbol{X} = {\\boldsymbol{\\mu}}^{\\top}\n\\]\nWe shall use Lagrange Multiplier to solve such an optimization problem as follows.\n\\[\n\\begin{aligned}\n\\mathcal{L} &= -{\\boldsymbol{p}}^{\\top}\\ln(\\frac{1}{n\\boldsymbol{p}})\\boldsymbol{1} - \\lambda{({\\boldsymbol{p}}^{\\top}\\boldsymbol{X} - {\\boldsymbol{\\mu}}^{\\top})}^{\\top} \\\\\n&= {\\boldsymbol{p}}^{\\top}\\ln(n\\boldsymbol{p})\\boldsymbol{1} - \\lambda[( {\\boldsymbol{X}}^{\\top} - \\boldsymbol{\\mu}{\\boldsymbol{1}}^{\\top} )\\boldsymbol{p}].\n\\end{aligned}\n\\tag{1}\n\\]\nAbove equation could be solved by taking derivative w.r.t \\(\\boldsymbol{p}\\) and \\(\\lambda\\), we then equate both derivations to zero and solve such equations. First, Taking derivative \\(\\mathcal{L}\\) w.r.t \\(\\lambda\\) we obtain\n\\[\n{\\boldsymbol{p}}^{\\top}\\boldsymbol{X} = {\\boldsymbol{\\mu}}^{\\top}.\n\\tag{2}\n\\]\nNext, taking derivative w.r.t \\(p_i\\):\n\\[\n\\begin{aligned}\n&\\quad \\frac{\\partial\\mathcal{L}}{\\partial p_i} = 0 \\\\\n&\\Rightarrow \\ln(np_i) +n -\\lambda(x_i-\\mu) = 0 \\\\\n&\\Rightarrow \\ln(np_i) = \\lambda(x_i-\\mu)-n \\\\\n&\\Rightarrow p_i = \\frac{\\exp[\\lambda(x_i -\\mu)-n]}{n} \\\\\n&\\Rightarrow p_i = \\frac{e^{-n}}{n}\\exp[\\lambda(x_i-\\mu)]\n\\end{aligned}\n\\tag{3}\n\\]\nthus,\n\\[\n\\begin{aligned}\n&\\quad 1 = \\sum_{i=1}^np_i = \\frac{e^{-n}}{n}\\sum_{i=1}^n\\exp[\\lambda(x_i-\\mu)] \\\\\n& \\Rightarrow \\frac{n}{e^{-n}} = \\sum_{i=1}^n\\exp[\\lambda(x_i-\\mu)]\n\\end{aligned}\n\\tag{4}\n\\]\nsubstitute (4) to (3)`, we obtain\n\n\\[\\begin{equation}\np_i = \\frac{\\exp[\\lambda(x_i - \\mu)]}{\\sum_{i=1}^n\\exp[\\lambda(x_i-\\mu)]}\n\\tag{5}\n\\end{equation}\\]\n\n(5) indicates that \\(p_i\\) can be estimated by fitting logistic model in which \\(z_i= x_i-\\mu\\) is one covariate, and \\(z_i\\) must be statisfying (2).\n\n\n\n",
      "last_modified": "2022-01-23T02:20:10-05:00"
    }
  ],
  "collections": []
}
