---
title: "Bayesian Computational Approaches"
subtitle: "010101"
description: |
author:
  - name: Nam Anh
date: "Oct. 11, 2021"
bibliography: citation.bib
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
---

\newcommand{\v}[1]{\boldsymbol{#1}}
\newcommand{\hat}[1]{\widehat{#1}}
\newcommand{\mm}[1]{\mathbb{#1}}
\newcommand{\bar}[1]{\overline{#1}}

\def\E{\Bbb{E}}
\def\V{\Bbb{V}}
\def\P{\Bbb{P}}
\def\I{{\large\unicode{x1D7D9}}}
\def\indep{\perp\!\!\!\!\perp}
\newcommand{\overeq}[2]{\stackrel{#1}{#2}}
\def\epsilon{\varepsilon} 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Maximize posterior distribution

Let us define $p(\v{\theta}|\v{Y})$ be posterior distribution, and 

$$
\hat{\v{\theta}}_{MAP} = \arg\max_{\v\theta}\ln p(\v{\theta}|\v{Y}) = \arg\max_{\v\theta}\ln f(\v{Y}|\v{\theta}) + \ln \pi(\v\theta)
(\#eq:1)
$$
is the maximizer of $p(\v{\theta}|\v{Y})$. 

MAP estimator can be obtained by function `optim` in `R`. Its illustration in R will be pored over latter. 

# Numerical integration

Some summary statistics of interest can be written as integrals, so they can be approximated by summations. For example, expectation, variance and proportions of a distribution can be written as follows

$$
\begin{aligned}
\E(\theta|\v{Y}) &= \int \theta_1p(\theta|\v{Y})d\theta \\
\V(\theta|\v{Y}) &= \int [\theta - \E(\theta|\v{Y})]^2p(\theta|\v{Y})d\theta \\
\P(\theta > c|\v Y) &= \int_c^{\infty}p(\theta|\v{Y})d\theta
\end{aligned}
(\#eq:2)
$$

All three measures in \@ref(eq:2) can be rewritten as $\E[g(\theta)]$, for example, $\P(\theta > c| \v Y)$ is equivalent to 

$$
\int_{-\infty}^{\infty}\I_{\theta > c}(\theta)p(\theta|\v Y)d\theta
$$

Since integration can be approximated by summation, $\E[g(\theta)] = \int_{\Theta}g(\theta)p(\theta|\v Y)$ is equivalent to

$$
\sum_{j=1}^m g(\theta_j)W_j,
$$

where $W_j$ is the weight given to the grid point $j$. 

# Markov Chain Monte Carlo (MCMC)

Monte Carlo (MC) methods are appealing since they mimic the process of statistical concept where a sample is used to draw inferences of population. Two well-known measures are first two moments of distribution, i.e. $\E(X)$ and $\E(X^2)$.  
